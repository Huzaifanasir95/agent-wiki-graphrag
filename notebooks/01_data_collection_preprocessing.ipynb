{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee89297",
   "metadata": {},
   "source": [
    "# Phase 1: Data Collection & Preprocessing\n",
    "## Wikipedia Article Generator with GraphRAG\n",
    "\n",
    "**Objective**: Collect and preprocess a small Wikipedia dataset (<1GB) for building our knowledge graph\n",
    "\n",
    "**What we'll do in this notebook:**\n",
    "1. Set up environment and install required libraries\n",
    "2. Download Wikipedia articles using Wikipedia API (targeted topics)\n",
    "3. Parse and clean article content\n",
    "4. Extract entities and relationships\n",
    "5. Create vector embeddings\n",
    "6. Prepare data for knowledge graph construction\n",
    "\n",
    "**Dataset Strategy**: Instead of downloading the full 20GB Wikipedia dump, we'll:\n",
    "- Use Wikipedia API to fetch ~1000-2000 articles on specific topics\n",
    "- Focus on interconnected topics (e.g., Science, Technology, History)\n",
    "- Keep total size under 1GB\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c1b6c",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "First, let's install all the dependencies we need. Run this cell once at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ec047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell once, then restart kernel if needed\n",
    "\n",
    "!pip install wikipedia-api\n",
    "!pip install mwparserfromhell\n",
    "!pip install spacy\n",
    "!pip install sentence-transformers\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install tqdm\n",
    "!pip install beautifulsoup4\n",
    "!pip install networkx\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "\n",
    "# Download spaCy language model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad5ec4",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n",
    "\n",
    "Import all necessary libraries for data collection, processing, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717becea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Wikipedia API\n",
    "import wikipediaapi\n",
    "\n",
    "# Text processing\n",
    "import mwparserfromhell\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Graph & visualization\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626e1276",
   "metadata": {},
   "source": [
    "## Step 3: Setup Directory Structure\n",
    "\n",
    "Create necessary directories for storing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project paths\n",
    "PROJECT_ROOT = Path(r\"d:\\Projects\\agent-wiki-graphrag\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "EMBEDDINGS_DIR = DATA_DIR / \"embeddings\"\n",
    "KG_DIR = DATA_DIR / \"knowledge_graph\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [RAW_DIR, PROCESSED_DIR, EMBEDDINGS_DIR, KG_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"âœ“ Created/verified: {directory}\")\n",
    "\n",
    "# Define output file paths\n",
    "ARTICLES_FILE = RAW_DIR / \"wikipedia_articles.json\"\n",
    "ENTITIES_FILE = PROCESSED_DIR / \"entities.json\"\n",
    "RELATIONS_FILE = PROCESSED_DIR / \"relations.json\"\n",
    "EMBEDDINGS_FILE = EMBEDDINGS_DIR / \"article_embeddings.pkl\"\n",
    "METADATA_FILE = PROCESSED_DIR / \"metadata.json\"\n",
    "\n",
    "print(f\"\\nâœ“ Directory structure ready!\")\n",
    "print(f\"Data will be saved to: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aac071",
   "metadata": {},
   "source": [
    "## Step 4: Define Target Topics\n",
    "\n",
    "Define the topics we want to collect. We'll focus on interconnected domains to create a rich knowledge graph while staying under 1GB.\n",
    "\n",
    "**Strategy**: \n",
    "- Start with seed topics (e.g., \"Artificial Intelligence\", \"Machine Learning\")\n",
    "- Follow links to related articles (up to 2 hops)\n",
    "- Target ~1500-2000 articles total\n",
    "- Estimated size: 500MB-800MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a231cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define seed topics across different domains\n",
    "SEED_TOPICS = {\n",
    "    \"Technology\": [\n",
    "        \"Artificial intelligence\",\n",
    "        \"Machine learning\",\n",
    "        \"Natural language processing\",\n",
    "        \"Computer vision\",\n",
    "        \"Deep learning\",\n",
    "        \"Neural network\",\n",
    "        \"Transformer (machine learning model)\",\n",
    "        \"Large language model\"\n",
    "    ],\n",
    "    \"Science\": [\n",
    "        \"Quantum computing\",\n",
    "        \"Quantum mechanics\",\n",
    "        \"Theory of relativity\",\n",
    "        \"Particle physics\",\n",
    "        \"String theory\"\n",
    "    ],\n",
    "    \"Computing\": [\n",
    "        \"Algorithm\",\n",
    "        \"Data structure\",\n",
    "        \"Python (programming language)\",\n",
    "        \"Software engineering\",\n",
    "        \"Cloud computing\"\n",
    "    ],\n",
    "    \"Mathematics\": [\n",
    "        \"Linear algebra\",\n",
    "        \"Calculus\",\n",
    "        \"Graph theory\",\n",
    "        \"Statistics\",\n",
    "        \"Probability theory\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten seed topics\n",
    "seed_topics_list = [topic for topics in SEED_TOPICS.values() for topic in topics]\n",
    "\n",
    "print(f\"Total seed topics: {len(seed_topics_list)}\")\n",
    "print(f\"\\nSeed topics by domain:\")\n",
    "for domain, topics in SEED_TOPICS.items():\n",
    "    print(f\"  {domain}: {len(topics)} topics\")\n",
    "\n",
    "# Configuration\n",
    "MAX_ARTICLES = 2000  # Maximum number of articles to collect\n",
    "MAX_HOPS = 2  # How many link hops to follow\n",
    "MAX_SIZE_MB = 900  # Stop if we exceed this size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523a830",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Wikipedia API Client\n",
    "\n",
    "Set up the Wikipedia API client to fetch articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b64f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wikipedia API\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent='WikipediaArticleGenerator/1.0 (Educational Project)',\n",
    "    language='en',\n",
    "    extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "print(\"âœ“ Wikipedia API client initialized\")\n",
    "print(f\"Language: English\")\n",
    "print(f\"Format: Wiki markup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0852c1",
   "metadata": {},
   "source": [
    "## Step 6: Article Collection Functions\n",
    "\n",
    "Create functions to fetch articles and follow links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5958567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaCollector:\n",
    "    \"\"\"Collect Wikipedia articles with BFS link following\"\"\"\n",
    "    \n",
    "    def __init__(self, wiki_client, max_articles=2000, max_size_mb=900):\n",
    "        self.wiki = wiki_client\n",
    "        self.max_articles = max_articles\n",
    "        self.max_size_mb = max_size_mb\n",
    "        self.collected_articles = {}\n",
    "        self.visited_titles = set()\n",
    "        self.current_size_mb = 0\n",
    "        \n",
    "    def get_article(self, title):\n",
    "        \"\"\"Fetch a single article\"\"\"\n",
    "        try:\n",
    "            page = self.wiki.page(title)\n",
    "            if not page.exists():\n",
    "                return None\n",
    "            \n",
    "            # Extract article data\n",
    "            article_data = {\n",
    "                'title': page.title,\n",
    "                'url': page.fullurl,\n",
    "                'text': page.text,\n",
    "                'summary': page.summary,\n",
    "                'categories': [cat for cat in page.categories.keys()],\n",
    "                'links': [link for link in list(page.links.keys())[:50]],  # Limit links\n",
    "                'sections': [section.title for section in page.sections],\n",
    "                'length': len(page.text),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Estimate size\n",
    "            size_mb = len(json.dumps(article_data)) / (1024 * 1024)\n",
    "            \n",
    "            return article_data, size_mb\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {title}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def collect_articles(self, seed_topics, max_hops=2):\n",
    "        \"\"\"Collect articles using BFS from seed topics\"\"\"\n",
    "        \n",
    "        # Initialize queue with seed topics\n",
    "        queue = [(topic, 0) for topic in seed_topics]  # (title, hop_level)\n",
    "        \n",
    "        with tqdm(total=self.max_articles, desc=\"Collecting articles\") as pbar:\n",
    "            while queue and len(self.collected_articles) < self.max_articles:\n",
    "                \n",
    "                # Check size limit\n",
    "                if self.current_size_mb >= self.max_size_mb:\n",
    "                    print(f\"\\nâš  Size limit reached: {self.current_size_mb:.2f} MB\")\n",
    "                    break\n",
    "                \n",
    "                title, hop_level = queue.pop(0)\n",
    "                \n",
    "                # Skip if already visited\n",
    "                if title in self.visited_titles:\n",
    "                    continue\n",
    "                \n",
    "                self.visited_titles.add(title)\n",
    "                \n",
    "                # Fetch article\n",
    "                result = self.get_article(title)\n",
    "                if result is None:\n",
    "                    continue\n",
    "                \n",
    "                article_data, size_mb = result\n",
    "                \n",
    "                # Store article\n",
    "                self.collected_articles[title] = article_data\n",
    "                self.current_size_mb += size_mb\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    'size_mb': f'{self.current_size_mb:.1f}',\n",
    "                    'hop': hop_level\n",
    "                })\n",
    "                \n",
    "                # Add linked articles to queue (if within hop limit)\n",
    "                if hop_level < max_hops:\n",
    "                    for link in article_data['links'][:10]:  # Limit links per article\n",
    "                        if link not in self.visited_titles:\n",
    "                            queue.append((link, hop_level + 1))\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(0.1)\n",
    "        \n",
    "        print(f\"\\nâœ“ Collection complete!\")\n",
    "        print(f\"  Articles collected: {len(self.collected_articles)}\")\n",
    "        print(f\"  Total size: {self.current_size_mb:.2f} MB\")\n",
    "        \n",
    "        return self.collected_articles\n",
    "\n",
    "# Initialize collector\n",
    "collector = WikipediaCollector(wiki_wiki, MAX_ARTICLES, MAX_SIZE_MB)\n",
    "print(\"âœ“ Article collector initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92433306",
   "metadata": {},
   "source": [
    "## Step 7: Collect Wikipedia Articles\n",
    "\n",
    "Now let's collect the articles! This will take ~10-20 minutes depending on your internet connection.\n",
    "\n",
    "**Note**: If you want to test faster, reduce `MAX_ARTICLES` to 100-200 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ea1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start collection\n",
    "print(f\"Starting collection at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Target: {MAX_ARTICLES} articles, max {MAX_SIZE_MB} MB\\n\")\n",
    "\n",
    "articles = collector.collect_articles(seed_topics_list, max_hops=MAX_HOPS)\n",
    "\n",
    "# Save to file\n",
    "with open(ARTICLES_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Articles saved to: {ARTICLES_FILE}\")\n",
    "print(f\"File size: {ARTICLES_FILE.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420d082",
   "metadata": {},
   "source": [
    "## Step 8: Explore Collected Data\n",
    "\n",
    "Let's examine what we've collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b731884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(f\"Total articles collected: {len(articles)}\")\n",
    "print(f\"\\nArticle statistics:\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "articles_df = pd.DataFrame([\n",
    "    {\n",
    "        'title': title,\n",
    "        'length': data['length'],\n",
    "        'num_categories': len(data['categories']),\n",
    "        'num_links': len(data['links']),\n",
    "        'num_sections': len(data['sections'])\n",
    "    }\n",
    "    for title, data in articles.items()\n",
    "])\n",
    "\n",
    "print(articles_df.describe())\n",
    "\n",
    "# Show sample articles\n",
    "print(f\"\\n\\nSample articles:\")\n",
    "print(articles_df[['title', 'length', 'num_sections']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize article length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Article length histogram\n",
    "axes[0].hist(articles_df['length'], bins=50, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Article Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Article Lengths')\n",
    "axes[0].axvline(articles_df['length'].median(), color='red', linestyle='--', \n",
    "                label=f'Median: {articles_df[\"length\"].median():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Number of sections\n",
    "axes[1].hist(articles_df['num_sections'], bins=30, color='coral', edgecolor='black')\n",
    "axes[1].set_xlabel('Number of Sections')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Article Sections')\n",
    "axes[1].axvline(articles_df['num_sections'].median(), color='red', linestyle='--',\n",
    "                label=f'Median: {articles_df[\"num_sections\"].median():.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb60a4d",
   "metadata": {},
   "source": [
    "## Step 9: Text Preprocessing\n",
    "\n",
    "Clean and preprocess the article text for entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a3346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wikipedia_text(text):\n",
    "    \"\"\"Clean Wikipedia text markup\"\"\"\n",
    "    # Remove citations [1], [2], etc.\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    \n",
    "    # Remove multiple newlines\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Clean all articles\n",
    "print(\"Cleaning article text...\")\n",
    "for title, data in tqdm(articles.items(), desc=\"Cleaning\"):\n",
    "    data['text_clean'] = clean_wikipedia_text(data['text'])\n",
    "    data['summary_clean'] = clean_wikipedia_text(data['summary'])\n",
    "\n",
    "print(\"âœ“ Text cleaning complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ce95b",
   "metadata": {},
   "source": [
    "## Step 10: Entity Extraction with spaCy\n",
    "\n",
    "Extract named entities (people, organizations, locations, etc.) from articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "print(\"Loading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"âœ“ spaCy model loaded\")\n",
    "\n",
    "# Process articles and extract entities\n",
    "all_entities = {}\n",
    "entity_types = defaultdict(int)\n",
    "\n",
    "print(\"\\nExtracting entities from articles...\")\n",
    "for title, data in tqdm(list(articles.items())[:100], desc=\"Processing\"):  # Start with 100 for speed\n",
    "    # Process text (use summary for speed, or text_clean for completeness)\n",
    "    doc = nlp(data['summary_clean'][:10000])  # Limit to first 10k chars\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append({\n",
    "            'text': ent.text,\n",
    "            'label': ent.label_,\n",
    "            'start': ent.start_char,\n",
    "            'end': ent.end_char\n",
    "        })\n",
    "        entity_types[ent.label_] += 1\n",
    "    \n",
    "    all_entities[title] = entities\n",
    "    data['entities'] = entities\n",
    "\n",
    "print(f\"\\nâœ“ Entity extraction complete\")\n",
    "print(f\"Articles processed: {len(all_entities)}\")\n",
    "print(f\"Total entity mentions: {sum(len(e) for e in all_entities.values())}\")\n",
    "print(f\"\\nEntity types found:\")\n",
    "for ent_type, count in sorted(entity_types.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {ent_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebb95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entity types\n",
    "plt.figure(figsize=(12, 6))\n",
    "entity_df = pd.DataFrame(list(entity_types.items()), columns=['Entity Type', 'Count'])\n",
    "entity_df = entity_df.sort_values('Count', ascending=False)\n",
    "\n",
    "plt.bar(entity_df['Entity Type'], entity_df['Count'], color='teal', edgecolor='black')\n",
    "plt.xlabel('Entity Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Named Entity Types')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show example entities\n",
    "example_article = list(all_entities.keys())[0]\n",
    "print(f\"\\nExample entities from '{example_article}':\")\n",
    "for ent in all_entities[example_article][:10]:\n",
    "    print(f\"  {ent['text']:30s} -> {ent['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0092aa8a",
   "metadata": {},
   "source": [
    "## Step 11: Create Vector Embeddings\n",
    "\n",
    "Generate embeddings for semantic search using sentence-transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646622e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer model\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"âœ“ Embedding model loaded\")\n",
    "print(f\"Model: all-MiniLM-L6-v2 (384 dimensions)\")\n",
    "\n",
    "# Create embeddings for article summaries\n",
    "print(\"\\nGenerating embeddings...\")\n",
    "article_embeddings = {}\n",
    "\n",
    "for title, data in tqdm(articles.items(), desc=\"Embedding articles\"):\n",
    "    # Embed the summary (faster) or full text\n",
    "    text_to_embed = data['summary_clean']\n",
    "    embedding = embedding_model.encode(text_to_embed)\n",
    "    article_embeddings[title] = embedding.tolist()\n",
    "\n",
    "print(f\"âœ“ Generated embeddings for {len(article_embeddings)} articles\")\n",
    "print(f\"Embedding dimension: {len(list(article_embeddings.values())[0])}\")\n",
    "\n",
    "# Save embeddings\n",
    "with open(EMBEDDINGS_FILE, 'wb') as f:\n",
    "    pickle.dump(article_embeddings, f)\n",
    "    \n",
    "print(f\"âœ“ Embeddings saved to: {EMBEDDINGS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590cf6f7",
   "metadata": {},
   "source": [
    "## Step 12: Build Initial Knowledge Graph Structure\n",
    "\n",
    "Create a graph representation of articles and their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5147854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph using NetworkX (will migrate to Neo4j later)\n",
    "print(\"Building knowledge graph...\")\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add article nodes\n",
    "for title, data in tqdm(articles.items(), desc=\"Adding nodes\"):\n",
    "    G.add_node(title, \n",
    "               node_type='article',\n",
    "               length=data['length'],\n",
    "               num_sections=len(data['sections']),\n",
    "               categories=data['categories'][:5])  # Limit categories\n",
    "\n",
    "# Add link edges\n",
    "edge_count = 0\n",
    "for title, data in tqdm(articles.items(), desc=\"Adding edges\"):\n",
    "    for link in data['links']:\n",
    "        if link in articles:  # Only link to articles in our collection\n",
    "            G.add_edge(title, link, relationship='links_to')\n",
    "            edge_count += 1\n",
    "\n",
    "print(f\"\\nâœ“ Knowledge graph created\")\n",
    "print(f\"  Nodes (articles): {G.number_of_nodes()}\")\n",
    "print(f\"  Edges (links): {G.number_of_edges()}\")\n",
    "print(f\"  Average degree: {edge_count / G.number_of_nodes():.2f}\")\n",
    "\n",
    "# Graph statistics\n",
    "print(f\"\\n  Density: {nx.density(G):.4f}\")\n",
    "if nx.is_weakly_connected(G):\n",
    "    print(f\"  Graph is connected\")\n",
    "else:\n",
    "    components = list(nx.weakly_connected_components(G))\n",
    "    print(f\"  Number of components: {len(components)}\")\n",
    "    print(f\"  Largest component size: {len(max(components, key=len))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize graph structure (sample)\n",
    "print(\"Creating graph visualization...\")\n",
    "\n",
    "# Get a subgraph for visualization (full graph would be too dense)\n",
    "seed_article = seed_topics_list[0]\n",
    "if seed_article in G:\n",
    "    # Get ego graph (article + neighbors)\n",
    "    subgraph = nx.ego_graph(G, seed_article, radius=1)\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    pos = nx.spring_layout(subgraph, k=2, iterations=50)\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(subgraph, pos, \n",
    "                          node_color='lightblue',\n",
    "                          node_size=500,\n",
    "                          alpha=0.8)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(subgraph, pos,\n",
    "                          edge_color='gray',\n",
    "                          alpha=0.5,\n",
    "                          arrows=True,\n",
    "                          arrowsize=10,\n",
    "                          arrowstyle='->')\n",
    "    \n",
    "    # Draw labels (truncate long titles)\n",
    "    labels = {node: node[:30] + '...' if len(node) > 30 else node \n",
    "              for node in subgraph.nodes()}\n",
    "    nx.draw_networkx_labels(subgraph, pos, labels, font_size=8)\n",
    "    \n",
    "    plt.title(f'Knowledge Graph: \"{seed_article}\" and Connected Articles')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ“ Visualized subgraph with {subgraph.number_of_nodes()} nodes\")\n",
    "else:\n",
    "    print(f\"Seed article '{seed_article}' not found in graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb39cb0",
   "metadata": {},
   "source": [
    "## Step 13: Save Processed Data\n",
    "\n",
    "Save all processed data for use in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated articles with entities\n",
    "with open(ARTICLES_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ“ Updated articles saved: {ARTICLES_FILE}\")\n",
    "\n",
    "# Save entities separately\n",
    "with open(ENTITIES_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_entities, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ“ Entities saved: {ENTITIES_FILE}\")\n",
    "\n",
    "# Save graph\n",
    "graph_file = KG_DIR / \"article_graph.gpickle\"\n",
    "nx.write_gpickle(G, graph_file)\n",
    "print(f\"âœ“ Graph saved: {graph_file}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'collection_date': datetime.now().isoformat(),\n",
    "    'total_articles': len(articles),\n",
    "    'total_entities': sum(len(e) for e in all_entities.values()),\n",
    "    'graph_nodes': G.number_of_nodes(),\n",
    "    'graph_edges': G.number_of_edges(),\n",
    "    'seed_topics': seed_topics_list,\n",
    "    'max_hops': MAX_HOPS,\n",
    "    'file_size_mb': ARTICLES_FILE.stat().st_size / (1024*1024)\n",
    "}\n",
    "\n",
    "with open(METADATA_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"âœ“ Metadata saved: {METADATA_FILE}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Phase 1 Complete! ðŸŽ‰\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Articles collected: {metadata['total_articles']}\")\n",
    "print(f\"  Entities extracted: {metadata['total_entities']}\")\n",
    "print(f\"  Graph nodes: {metadata['graph_nodes']}\")\n",
    "print(f\"  Graph edges: {metadata['graph_edges']}\")\n",
    "print(f\"  Total data size: {metadata['file_size_mb']:.2f} MB\")\n",
    "print(f\"\\nData saved to: {DATA_DIR}\")\n",
    "print(f\"\\nNext: Open notebook 02_graphrag_engine.ipynb to build the GraphRAG system!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
