{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcc7fa0",
   "metadata": {},
   "source": [
    "# Phase 2: GraphRAG Engine Development\n",
    "## Hybrid Retrieval System (Graph + Vector Search)\n",
    "\n",
    "**Objective**: Build a powerful hybrid retrieval system that combines:\n",
    "1. **Graph-based retrieval** - Navigate knowledge graph relationships\n",
    "2. **Vector semantic search** - Find similar content using embeddings\n",
    "3. **Fusion ranking** - Combine both approaches for better results\n",
    "\n",
    "**What we'll do in this notebook:**\n",
    "1. Load the processed data from Phase 1\n",
    "2. Build a FAISS vector index for fast similarity search\n",
    "3. Implement graph traversal algorithms (BFS, multi-hop reasoning)\n",
    "4. Create hybrid retrieval that fuses graph + vector results\n",
    "5. Test and evaluate retrieval quality\n",
    "6. Build a query interface\n",
    "\n",
    "**Prerequisites**: Complete notebook 01_data_collection_preprocessing.ipynb first!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f78bfe",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "Import all required libraries for building the GraphRAG engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be4483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Apps\\Python\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "âœ“ All libraries imported successfully!\n",
      "Working directory: d:\\Projects\\agent-wiki-graphrag\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "# Graph processing\n",
    "import networkx as nx\n",
    "\n",
    "# Vector search\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee9ab09",
   "metadata": {},
   "source": [
    "## Step 2: Load Processed Data\n",
    "\n",
    "Load all the data we prepared in Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4263563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from Phase 1...\n",
      "âœ“ Loaded 1337 articles\n",
      "âœ“ Loaded entities for 100 articles\n",
      "âœ“ Loaded 1337 article embeddings\n",
      "âœ“ Loaded knowledge graph: 1337 nodes, 11091 edges\n",
      "âœ“ Loaded metadata\n",
      "\n",
      "============================================================\n",
      "Data Summary:\n",
      "  Articles: 1337\n",
      "  Graph nodes: 1337\n",
      "  Graph edges: 11091\n",
      "  Embedding dimension: 384\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "PROJECT_ROOT = Path(r\"d:\\Projects\\agent-wiki-graphrag\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "EMBEDDINGS_DIR = DATA_DIR / \"embeddings\"\n",
    "KG_DIR = DATA_DIR / \"knowledge_graph\"\n",
    "\n",
    "# File paths\n",
    "ARTICLES_FILE = RAW_DIR / \"wikipedia_articles.json\"\n",
    "ENTITIES_FILE = PROCESSED_DIR / \"entities.json\"\n",
    "EMBEDDINGS_FILE = EMBEDDINGS_DIR / \"article_embeddings.pkl\"\n",
    "GRAPH_FILE = KG_DIR / \"article_graph.pkl\"\n",
    "METADATA_FILE = PROCESSED_DIR / \"metadata.json\"\n",
    "\n",
    "print(\"Loading data from Phase 1...\")\n",
    "\n",
    "# Load articles\n",
    "with open(ARTICLES_FILE, 'r', encoding='utf-8') as f:\n",
    "    articles = json.load(f)\n",
    "print(f\"âœ“ Loaded {len(articles)} articles\")\n",
    "\n",
    "# Load entities\n",
    "with open(ENTITIES_FILE, 'r', encoding='utf-8') as f:\n",
    "    entities = json.load(f)\n",
    "print(f\"âœ“ Loaded entities for {len(entities)} articles\")\n",
    "\n",
    "# Load embeddings\n",
    "with open(EMBEDDINGS_FILE, 'rb') as f:\n",
    "    article_embeddings = pickle.load(f)\n",
    "print(f\"âœ“ Loaded {len(article_embeddings)} article embeddings\")\n",
    "\n",
    "# Load knowledge graph\n",
    "with open(GRAPH_FILE, 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "print(f\"âœ“ Loaded knowledge graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "\n",
    "# Load metadata\n",
    "with open(METADATA_FILE, 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "print(f\"âœ“ Loaded metadata\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Data Summary:\")\n",
    "print(f\"  Articles: {len(articles)}\")\n",
    "print(f\"  Graph nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Graph edges: {G.number_of_edges()}\")\n",
    "print(f\"  Embedding dimension: {len(list(article_embeddings.values())[0])}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3f822",
   "metadata": {},
   "source": [
    "## Step 3: Build FAISS Vector Index\n",
    "\n",
    "Create an efficient FAISS index for fast similarity search over our article embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f888ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS vector index...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in method 'fvec_renorm_L2', argument 3 of type 'float *'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_4360\\2806011264.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     53\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m     54\u001b[39m \n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Build the index\u001b[39;00m\n\u001b[32m     56\u001b[39m print(\u001b[33m\"Building FAISS vector index...\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m vector_index = VectorIndex(article_embeddings)\n\u001b[32m     58\u001b[39m print(\u001b[33m\"âœ“ Vector index ready for search!\"\u001b[39m)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_4360\\2806011264.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, embeddings_dict)\u001b[39m\n\u001b[32m     15\u001b[39m         \u001b[38;5;66;03m# Create FAISS index\u001b[39;00m\n\u001b[32m     16\u001b[39m         self.index = faiss.IndexFlatIP(self.dimension)  \u001b[38;5;66;03m# Inner product (cosine similarity)\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m         \u001b[38;5;66;03m# Normalize embeddings for cosine similarity\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         faiss.normalize_L2(self.embeddings)\n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m         \u001b[38;5;66;03m# Add to index\u001b[39;00m\n\u001b[32m     22\u001b[39m         self.index.add(self.embeddings)\n",
      "\u001b[32md:\\Apps\\Python\\Lib\\site-packages\\faiss\\extra_wrappers.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m normalize_L2(x):\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     fvec_renorm_L2(x.shape[\u001b[32m1\u001b[39m], x.shape[\u001b[32m0\u001b[39m], swig_ptr(x))\n",
      "\u001b[32md:\\Apps\\Python\\Lib\\site-packages\\faiss\\swigfaiss_avx2.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(d, nx, x)\u001b[39m\n\u001b[32m   1413\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m fvec_renorm_L2(d, nx, x):\n\u001b[32m-> \u001b[39m\u001b[32m1414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _swigfaiss_avx2.fvec_renorm_L2(d, nx, x)\n",
      "\u001b[31mTypeError\u001b[39m: in method 'fvec_renorm_L2', argument 3 of type 'float *'"
     ]
    }
   ],
   "source": [
    "class VectorIndex:\n",
    "    \"\"\"FAISS-based vector index for semantic search\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_dict):\n",
    "        \"\"\"\n",
    "        Initialize vector index from embeddings dictionary\n",
    "        \n",
    "        Args:\n",
    "            embeddings_dict: Dict mapping article titles to embedding vectors\n",
    "        \"\"\"\n",
    "        self.titles = list(embeddings_dict.keys())\n",
    "        embeddings_list = [embeddings_dict[title] for title in self.titles]\n",
    "        self.embeddings = np.array(embeddings_list, dtype=np.float32)  # Ensure float32\n",
    "        self.dimension = self.embeddings.shape[1]\n",
    "        \n",
    "        # Create FAISS index\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)  # Inner product (cosine similarity)\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity (need contiguous array)\n",
    "        embeddings_copy = np.ascontiguousarray(self.embeddings)\n",
    "        faiss.normalize_L2(embeddings_copy)\n",
    "        \n",
    "        # Add to index\n",
    "        self.index.add(embeddings_copy)\n",
    "        \n",
    "        print(f\"âœ“ FAISS index built\")\n",
    "        print(f\"  Dimension: {self.dimension}\")\n",
    "        print(f\"  Total vectors: {self.index.ntotal}\")\n",
    "        print(f\"  Index type: Flat (exact search)\")\n",
    "    \n",
    "    def search(self, query_embedding, top_k=10):\n",
    "        \"\"\"\n",
    "        Search for similar articles\n",
    "        \n",
    "        Args:\n",
    "            query_embedding: Query vector (will be normalized)\n",
    "            top_k: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (title, score) tuples\n",
    "        \"\"\"\n",
    "        # Normalize query\n",
    "        query_vec = np.array([query_embedding], dtype=np.float32)\n",
    "        query_vec = np.ascontiguousarray(query_vec)\n",
    "        faiss.normalize_L2(query_vec)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_vec, top_k)\n",
    "        \n",
    "        # Return results\n",
    "        results = []\n",
    "        for idx, score in zip(indices[0], scores[0]):\n",
    "            if idx < len(self.titles):\n",
    "                results.append((self.titles[idx], float(score)))\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Build the index\n",
    "print(\"Building FAISS vector index...\")\n",
    "vector_index = VectorIndex(article_embeddings)\n",
    "print(\"âœ“ Vector index ready for search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b36a884",
   "metadata": {},
   "source": [
    "## Step 4: Load Embedding Model\n",
    "\n",
    "Load the same sentence transformer model we used in Phase 1 for encoding queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85782fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model for encoding queries\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"âœ“ Model loaded: all-MiniLM-L6-v2\")\n",
    "print(f\"âœ“ Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415f587",
   "metadata": {},
   "source": [
    "## Step 5: Test Vector Search\n",
    "\n",
    "Let's test the vector search with some sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fa2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vector search\n",
    "test_queries = [\n",
    "    \"neural networks and deep learning\",\n",
    "    \"quantum mechanics and physics\",\n",
    "    \"programming languages and software\",\n",
    "    \"statistics and probability theory\"\n",
    "]\n",
    "\n",
    "print(\"Testing Vector Search\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    \n",
    "    # Encode query\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    \n",
    "    # Search\n",
    "    results = vector_index.search(query_embedding, top_k=5)\n",
    "    \n",
    "    print(\"Top 5 results:\")\n",
    "    for i, (title, score) in enumerate(results, 1):\n",
    "        print(f\"  {i}. {title:45s} (score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7487f31d",
   "metadata": {},
   "source": [
    "## Step 6: Implement Graph Traversal\n",
    "\n",
    "Create functions for navigating the knowledge graph to find related articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRetriever:\n",
    "    \"\"\"Graph-based retrieval using NetworkX\"\"\"\n",
    "    \n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "        print(f\"âœ“ Graph retriever initialized\")\n",
    "        print(f\"  Nodes: {self.graph.number_of_nodes()}\")\n",
    "        print(f\"  Edges: {self.graph.number_of_edges()}\")\n",
    "    \n",
    "    def get_neighbors(self, node, max_neighbors=10):\n",
    "        \"\"\"Get direct neighbors of a node\"\"\"\n",
    "        if node not in self.graph:\n",
    "            return []\n",
    "        \n",
    "        # Get successors (outgoing edges)\n",
    "        neighbors = list(self.graph.successors(node))[:max_neighbors]\n",
    "        return neighbors\n",
    "    \n",
    "    def multi_hop_retrieval(self, start_nodes, max_hops=2, max_results=50):\n",
    "        \"\"\"\n",
    "        Multi-hop graph traversal using BFS\n",
    "        \n",
    "        Args:\n",
    "            start_nodes: List of starting article titles\n",
    "            max_hops: Maximum number of hops to traverse\n",
    "            max_results: Maximum number of articles to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (article_title, hop_distance) tuples\n",
    "        \"\"\"\n",
    "        if isinstance(start_nodes, str):\n",
    "            start_nodes = [start_nodes]\n",
    "        \n",
    "        visited = set()\n",
    "        results = []\n",
    "        queue = [(node, 0) for node in start_nodes if node in self.graph]\n",
    "        \n",
    "        while queue and len(results) < max_results:\n",
    "            current_node, current_hop = queue.pop(0)\n",
    "            \n",
    "            if current_node in visited:\n",
    "                continue\n",
    "            \n",
    "            visited.add(current_node)\n",
    "            results.append((current_node, current_hop))\n",
    "            \n",
    "            # Add neighbors if within hop limit\n",
    "            if current_hop < max_hops:\n",
    "                neighbors = self.get_neighbors(current_node)\n",
    "                for neighbor in neighbors:\n",
    "                    if neighbor not in visited:\n",
    "                        queue.append((neighbor, current_hop + 1))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_related_by_category(self, node, max_results=10):\n",
    "        \"\"\"Find articles in similar categories\"\"\"\n",
    "        if node not in self.graph:\n",
    "            return []\n",
    "        \n",
    "        node_data = self.graph.nodes[node]\n",
    "        node_categories = set(node_data.get('categories', []))\n",
    "        \n",
    "        if not node_categories:\n",
    "            return []\n",
    "        \n",
    "        # Find nodes with overlapping categories\n",
    "        related = []\n",
    "        for other_node in self.graph.nodes():\n",
    "            if other_node == node:\n",
    "                continue\n",
    "            \n",
    "            other_categories = set(self.graph.nodes[other_node].get('categories', []))\n",
    "            overlap = len(node_categories & other_categories)\n",
    "            \n",
    "            if overlap > 0:\n",
    "                related.append((other_node, overlap))\n",
    "        \n",
    "        # Sort by overlap and return top results\n",
    "        related.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [node for node, _ in related[:max_results]]\n",
    "\n",
    "# Initialize graph retriever\n",
    "print(\"Initializing graph retriever...\")\n",
    "graph_retriever = GraphRetriever(G)\n",
    "print(\"âœ“ Graph retriever ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f80ff1",
   "metadata": {},
   "source": [
    "## Step 7: Test Graph Retrieval\n",
    "\n",
    "Test the graph-based retrieval methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205de9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test graph retrieval\n",
    "test_article = \"Artificial intelligence\"\n",
    "\n",
    "if test_article in G:\n",
    "    print(f\"Testing Graph Retrieval for: '{test_article}'\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Direct neighbors\n",
    "    print(f\"\\nDirect neighbors (1-hop):\")\n",
    "    neighbors = graph_retriever.get_neighbors(test_article, max_neighbors=10)\n",
    "    for i, neighbor in enumerate(neighbors[:5], 1):\n",
    "        print(f\"  {i}. {neighbor}\")\n",
    "    \n",
    "    # Multi-hop retrieval\n",
    "    print(f\"\\nMulti-hop retrieval (2 hops, top 10):\")\n",
    "    multi_hop_results = graph_retriever.multi_hop_retrieval(test_article, max_hops=2, max_results=10)\n",
    "    for i, (article, hop) in enumerate(multi_hop_results, 1):\n",
    "        print(f\"  {i}. {article:45s} (hop: {hop})\")\n",
    "    \n",
    "    # Category-based\n",
    "    print(f\"\\nRelated by category:\")\n",
    "    category_related = graph_retriever.get_related_by_category(test_article, max_results=5)\n",
    "    for i, article in enumerate(category_related, 1):\n",
    "        print(f\"  {i}. {article}\")\n",
    "else:\n",
    "    print(f\"Article '{test_article}' not found in graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10649714",
   "metadata": {},
   "source": [
    "## Step 8: Build Hybrid Retriever\n",
    "\n",
    "Combine graph-based and vector-based retrieval with fusion ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f88443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRetriever:\n",
    "    \"\"\"Hybrid retrieval combining graph and vector search\"\"\"\n",
    "    \n",
    "    def __init__(self, graph_retriever, vector_index, embedding_model, articles):\n",
    "        self.graph = graph_retriever\n",
    "        self.vector = vector_index\n",
    "        self.encoder = embedding_model\n",
    "        self.articles = articles\n",
    "        print(\"âœ“ Hybrid retriever initialized\")\n",
    "    \n",
    "    def retrieve(self, query, top_k=10, alpha=0.5, use_graph=True, use_vector=True):\n",
    "        \"\"\"\n",
    "        Hybrid retrieval with fusion ranking\n",
    "        \n",
    "        Args:\n",
    "            query: Search query string\n",
    "            top_k: Number of results to return\n",
    "            alpha: Weight for fusion (0=graph only, 1=vector only, 0.5=equal)\n",
    "            use_graph: Whether to use graph retrieval\n",
    "            use_vector: Whether to use vector retrieval\n",
    "            \n",
    "        Returns:\n",
    "            List of (article_title, score, source) tuples\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Vector search\n",
    "        if use_vector:\n",
    "            query_embedding = self.encoder.encode(query)\n",
    "            vector_results = self.vector.search(query_embedding, top_k=top_k*2)\n",
    "            \n",
    "            for title, score in vector_results:\n",
    "                if title not in results:\n",
    "                    results[title] = {'vector_score': 0, 'graph_score': 0}\n",
    "                results[title]['vector_score'] = score\n",
    "        \n",
    "        # Graph search (if query matches an article)\n",
    "        if use_graph:\n",
    "            # Try to find articles matching query terms\n",
    "            query_terms = query.lower().split()\n",
    "            matching_articles = []\n",
    "            \n",
    "            for title in self.articles.keys():\n",
    "                title_lower = title.lower()\n",
    "                if any(term in title_lower for term in query_terms):\n",
    "                    matching_articles.append(title)\n",
    "            \n",
    "            # Multi-hop retrieval from matching articles\n",
    "            if matching_articles:\n",
    "                graph_results = self.graph.multi_hop_retrieval(\n",
    "                    matching_articles[:3],  # Use top 3 matches\n",
    "                    max_hops=2,\n",
    "                    max_results=top_k*2\n",
    "                )\n",
    "                \n",
    "                # Score based on hop distance (closer = higher score)\n",
    "                for title, hop in graph_results:\n",
    "                    if title not in results:\n",
    "                        results[title] = {'vector_score': 0, 'graph_score': 0}\n",
    "                    # Score: 1.0 for 0 hops, 0.5 for 1 hop, 0.25 for 2 hops\n",
    "                    results[title]['graph_score'] = 1.0 / (2 ** hop)\n",
    "        \n",
    "        # Normalize scores\n",
    "        if results:\n",
    "            max_vector = max((r['vector_score'] for r in results.values()), default=1.0)\n",
    "            max_graph = max((r['graph_score'] for r in results.values()), default=1.0)\n",
    "            \n",
    "            for title in results:\n",
    "                if max_vector > 0:\n",
    "                    results[title]['vector_score'] /= max_vector\n",
    "                if max_graph > 0:\n",
    "                    results[title]['graph_score'] /= max_graph\n",
    "        \n",
    "        # Fusion ranking\n",
    "        ranked_results = []\n",
    "        for title, scores in results.items():\n",
    "            # Weighted combination\n",
    "            if use_vector and use_graph:\n",
    "                final_score = alpha * scores['vector_score'] + (1 - alpha) * scores['graph_score']\n",
    "            elif use_vector:\n",
    "                final_score = scores['vector_score']\n",
    "            elif use_graph:\n",
    "                final_score = scores['graph_score']\n",
    "            else:\n",
    "                final_score = 0\n",
    "            \n",
    "            # Determine primary source\n",
    "            if scores['vector_score'] > scores['graph_score']:\n",
    "                source = 'vector'\n",
    "            elif scores['graph_score'] > scores['vector_score']:\n",
    "                source = 'graph'\n",
    "            else:\n",
    "                source = 'hybrid'\n",
    "            \n",
    "            ranked_results.append((title, final_score, source))\n",
    "        \n",
    "        # Sort by score and return top-k\n",
    "        ranked_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return ranked_results[:top_k]\n",
    "    \n",
    "    def explain_results(self, query, top_k=5):\n",
    "        \"\"\"Retrieve and explain the results\"\"\"\n",
    "        results = self.retrieve(query, top_k=top_k, alpha=0.5)\n",
    "        \n",
    "        print(f\"Query: '{query}'\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Rank':<6} {'Title':<45} {'Score':<8} {'Source':<10}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for i, (title, score, source) in enumerate(results, 1):\n",
    "            print(f\"{i:<6} {title[:44]:<45} {score:.4f}   {source:<10}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize hybrid retriever\n",
    "print(\"Building hybrid retriever...\")\n",
    "hybrid_retriever = HybridRetriever(graph_retriever, vector_index, embedding_model, articles)\n",
    "print(\"âœ“ Hybrid retriever ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c68345",
   "metadata": {},
   "source": [
    "## Step 9: Test Hybrid Retrieval\n",
    "\n",
    "Test the hybrid retrieval system with various queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1dd8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid retrieval\n",
    "test_queries = [\n",
    "    \"neural networks and deep learning\",\n",
    "    \"quantum computing applications\",\n",
    "    \"machine learning algorithms\",\n",
    "    \"artificial intelligence safety\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print()\n",
    "    hybrid_retriever.explain_results(query, top_k=5)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc68042",
   "metadata": {},
   "source": [
    "## Step 10: Compare Retrieval Methods\n",
    "\n",
    "Compare vector-only, graph-only, and hybrid retrieval side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d21ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_methods(query, top_k=5):\n",
    "    \"\"\"Compare different retrieval approaches\"\"\"\n",
    "    \n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Vector only\n",
    "    vector_results = hybrid_retriever.retrieve(query, top_k=top_k, use_graph=False, use_vector=True)\n",
    "    \n",
    "    # Graph only\n",
    "    graph_results = hybrid_retriever.retrieve(query, top_k=top_k, use_graph=True, use_vector=False)\n",
    "    \n",
    "    # Hybrid\n",
    "    hybrid_results = hybrid_retriever.retrieve(query, top_k=top_k, alpha=0.5)\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Rank': range(1, top_k+1),\n",
    "        'Vector Only': [r[0][:35] for r in vector_results],\n",
    "        'V-Score': [f\"{r[1]:.3f}\" for r in vector_results],\n",
    "        'Graph Only': [r[0][:35] if len(graph_results) > i else '' \n",
    "                       for i, r in enumerate(graph_results + [('', 0, '')] * top_k)][:top_k],\n",
    "        'G-Score': [f\"{r[1]:.3f}\" if len(graph_results) > i else '' \n",
    "                    for i, r in enumerate(graph_results + [('', 0, '')] * top_k)][:top_k],\n",
    "        'Hybrid': [r[0][:35] for r in hybrid_results],\n",
    "        'H-Score': [f\"{r[1]:.3f}\" for r in hybrid_results]\n",
    "    })\n",
    "    \n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Test with a query\n",
    "test_query = \"machine learning and neural networks\"\n",
    "compare_retrieval_methods(test_query, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ee211",
   "metadata": {},
   "source": [
    "## Step 11: Build Context Retrieval\n",
    "\n",
    "Retrieve full article context with citations for article generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7360128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextRetriever:\n",
    "    \"\"\"Retrieve full context with article content for generation\"\"\"\n",
    "    \n",
    "    def __init__(self, hybrid_retriever, articles):\n",
    "        self.retriever = hybrid_retriever\n",
    "        self.articles = articles\n",
    "    \n",
    "    def get_context(self, query, top_k=5, include_summary=True, include_text=False):\n",
    "        \"\"\"\n",
    "        Retrieve full context for a query\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of articles to retrieve\n",
    "            include_summary: Include article summaries\n",
    "            include_text: Include full article text (may be large)\n",
    "            \n",
    "        Returns:\n",
    "            List of context dictionaries\n",
    "        \"\"\"\n",
    "        # Get relevant articles\n",
    "        results = self.retriever.retrieve(query, top_k=top_k)\n",
    "        \n",
    "        context = []\n",
    "        for title, score, source in results:\n",
    "            if title in self.articles:\n",
    "                article = self.articles[title]\n",
    "                \n",
    "                ctx = {\n",
    "                    'title': title,\n",
    "                    'score': score,\n",
    "                    'source': source,\n",
    "                    'url': article.get('url', ''),\n",
    "                    'categories': article.get('categories', [])[:5]\n",
    "                }\n",
    "                \n",
    "                if include_summary:\n",
    "                    ctx['summary'] = article.get('summary_clean', article.get('summary', ''))\n",
    "                \n",
    "                if include_text:\n",
    "                    ctx['text'] = article.get('text_clean', article.get('text', ''))\n",
    "                \n",
    "                context.append(ctx)\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def format_context_for_llm(self, query, top_k=5):\n",
    "        \"\"\"Format context as a prompt for LLM\"\"\"\n",
    "        context = self.get_context(query, top_k=top_k, include_summary=True)\n",
    "        \n",
    "        prompt = f\"Query: {query}\\n\\n\"\n",
    "        prompt += \"Relevant Wikipedia Articles:\\n\"\n",
    "        prompt += \"=\"*80 + \"\\n\\n\"\n",
    "        \n",
    "        for i, ctx in enumerate(context, 1):\n",
    "            prompt += f\"{i}. {ctx['title']}\\n\"\n",
    "            prompt += f\"   URL: {ctx['url']}\\n\"\n",
    "            prompt += f\"   Summary: {ctx['summary'][:500]}...\\n\\n\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def get_related_entities(self, query, top_k=5):\n",
    "        \"\"\"Get entities from retrieved articles\"\"\"\n",
    "        results = self.retriever.retrieve(query, top_k=top_k)\n",
    "        \n",
    "        all_entities = defaultdict(int)\n",
    "        for title, score, source in results:\n",
    "            if title in self.articles:\n",
    "                article_entities = self.articles[title].get('entities', [])\n",
    "                for ent in article_entities:\n",
    "                    all_entities[ent['text']] += 1\n",
    "        \n",
    "        # Return most common entities\n",
    "        sorted_entities = sorted(all_entities.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_entities[:20]\n",
    "\n",
    "# Initialize context retriever\n",
    "context_retriever = ContextRetriever(hybrid_retriever, articles)\n",
    "print(\"âœ“ Context retriever ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b62f6b",
   "metadata": {},
   "source": [
    "## Step 12: Test Context Retrieval\n",
    "\n",
    "Test retrieving full context for article generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test context retrieval\n",
    "test_query = \"deep learning applications\"\n",
    "\n",
    "print(f\"Retrieving context for: '{test_query}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get context\n",
    "context = context_retriever.get_context(test_query, top_k=3)\n",
    "\n",
    "for i, ctx in enumerate(context, 1):\n",
    "    print(f\"\\n{i}. {ctx['title']}\")\n",
    "    print(f\"   Score: {ctx['score']:.4f} | Source: {ctx['source']}\")\n",
    "    print(f\"   URL: {ctx['url']}\")\n",
    "    print(f\"   Categories: {', '.join(ctx['categories'][:3])}\")\n",
    "    print(f\"   Summary: {ctx['summary'][:300]}...\")\n",
    "\n",
    "# Get related entities\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Related Entities:\")\n",
    "entities = context_retriever.get_related_entities(test_query, top_k=5)\n",
    "for ent, count in entities[:10]:\n",
    "    print(f\"  {ent:30s} (mentions: {count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab484485",
   "metadata": {},
   "source": [
    "## Step 13: Save GraphRAG Engine\n",
    "\n",
    "Save the complete GraphRAG engine for use in Phase 3 (Agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01387b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the GraphRAG components\n",
    "print(\"Saving GraphRAG engine components...\")\n",
    "\n",
    "# Save FAISS index\n",
    "faiss_index_file = EMBEDDINGS_DIR / \"faiss_index.bin\"\n",
    "faiss.write_index(vector_index.index, str(faiss_index_file))\n",
    "print(f\"âœ“ FAISS index saved: {faiss_index_file}\")\n",
    "\n",
    "# Save title mapping\n",
    "titles_file = EMBEDDINGS_DIR / \"index_titles.json\"\n",
    "with open(titles_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(vector_index.titles, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ“ Title mapping saved: {titles_file}\")\n",
    "\n",
    "# Save retriever configuration\n",
    "config = {\n",
    "    'vector_dimension': vector_index.dimension,\n",
    "    'total_articles': len(vector_index.titles),\n",
    "    'embedding_model': 'all-MiniLM-L6-v2',\n",
    "    'graph_nodes': G.number_of_nodes(),\n",
    "    'graph_edges': G.number_of_edges(),\n",
    "    'created_at': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "config_file = PROCESSED_DIR / \"graphrag_config.json\"\n",
    "with open(config_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"âœ“ Configuration saved: {config_file}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Phase 2 Complete! ðŸŽ‰\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nGraphRAG Engine Summary:\")\n",
    "print(f\"  Vector Index: {config['total_articles']} articles, {config['vector_dimension']}D\")\n",
    "print(f\"  Knowledge Graph: {config['graph_nodes']} nodes, {config['graph_edges']} edges\")\n",
    "print(f\"  Embedding Model: {config['embedding_model']}\")\n",
    "print(f\"\\nComponents saved to: {DATA_DIR}\")\n",
    "print(f\"\\nNext: Open notebook 03_agent_system.ipynb to build the multi-agent system!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
