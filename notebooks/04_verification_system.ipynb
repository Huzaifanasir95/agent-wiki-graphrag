{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714404b7",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097c146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Apps\\Python\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "✓ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP libraries\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Graph and vector search\n",
    "import networkx as nx\n",
    "import faiss\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path('d:/Projects/agent-wiki-graphrag')\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "EMBEDDINGS_DIR = DATA_DIR / 'embeddings'\n",
    "KG_DIR = DATA_DIR / 'knowledge_graph'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'\n",
    "ARTICLES_DIR = OUTPUTS_DIR / 'articles'\n",
    "VERIFICATION_DIR = OUTPUTS_DIR / 'verification'\n",
    "VERIFICATION_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3d35e",
   "metadata": {},
   "source": [
    "## 2. Load Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a036ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "✓ Loaded 1337 articles\n",
      "✓ Loaded knowledge graph: 1337 nodes, 11091 edges\n",
      "✓ Loaded FAISS index: 1337 vectors\n",
      "✓ Loaded 100 articles with entities\n",
      "\n",
      "============================================================\n",
      "Data loaded successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load processed data from previous phases\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "ARTICLES_FILE = RAW_DIR / 'wikipedia_articles.json'\n",
    "ENTITIES_FILE = PROCESSED_DIR / 'entities.json'\n",
    "EMBEDDINGS_FILE = EMBEDDINGS_DIR / 'article_embeddings.pkl'\n",
    "GRAPH_FILE = KG_DIR / 'article_graph.pkl'\n",
    "FAISS_INDEX_FILE = EMBEDDINGS_DIR / 'faiss_index.bin'\n",
    "INDEX_TITLES_FILE = EMBEDDINGS_DIR / 'index_titles.json'\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Load articles\n",
    "with open(ARTICLES_FILE, 'r', encoding='utf-8') as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "# Load entities\n",
    "with open(ENTITIES_FILE, 'r', encoding='utf-8') as f:\n",
    "    entities = json.load(f)\n",
    "\n",
    "# Load embeddings\n",
    "with open(EMBEDDINGS_FILE, 'rb') as f:\n",
    "    article_embeddings = pickle.load(f)\n",
    "\n",
    "# Load knowledge graph\n",
    "with open(GRAPH_FILE, 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "# Load FAISS index\n",
    "faiss_index = faiss.read_index(str(FAISS_INDEX_FILE))\n",
    "\n",
    "# Load index titles\n",
    "with open(INDEX_TITLES_FILE, 'r', encoding='utf-8') as f:\n",
    "    index_titles = json.load(f)\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load spaCy for claim extraction\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(f\"✓ Loaded {len(articles)} articles\")\n",
    "print(f\"✓ Loaded knowledge graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "print(f\"✓ Loaded FAISS index: {faiss_index.ntotal} vectors\")\n",
    "print(f\"✓ Loaded {len(entities)} articles with entities\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Data loaded successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4fe829",
   "metadata": {},
   "source": [
    "## 3. Claim Extraction\n",
    "\n",
    "Extract factual claims from generated articles that can be verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc15da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Claim extractor initialized\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Claim:\n",
    "    \"\"\"Represents a factual claim\"\"\"\n",
    "    text: str\n",
    "    claim_type: str  # 'entity', 'relationship', 'attribute', 'numerical'\n",
    "    entities: List[str] = field(default_factory=list)\n",
    "    context: str = \"\"\n",
    "    section: str = \"\"\n",
    "    confidence: float = 0.0\n",
    "    evidence: List[Dict] = field(default_factory=list)\n",
    "    verification_status: str = \"pending\"  # 'pending', 'verified', 'refuted', 'uncertain'\n",
    "\n",
    "class ClaimExtractor:\n",
    "    \"\"\"Extract verifiable claims from text\"\"\"\n",
    "    \n",
    "    def __init__(self, nlp_model):\n",
    "        self.nlp = nlp_model\n",
    "        \n",
    "        # Patterns for different claim types\n",
    "        self.numerical_pattern = re.compile(r'\\b\\d+[\\d,\\.]*\\s*(?:percent|%|million|billion|thousand|km|miles|years?|days?|months?)\\b', re.IGNORECASE)\n",
    "        self.date_pattern = re.compile(r'\\b(?:in|since|during|by)\\s+\\d{4}\\b')\n",
    "        \n",
    "    def extract_claims(self, text: str, section: str = \"\") -> List[Claim]:\n",
    "        \"\"\"Extract claims from text\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        claims = []\n",
    "        \n",
    "        # Extract claims from sentences\n",
    "        for sent in doc.sents:\n",
    "            sent_text = sent.text.strip()\n",
    "            \n",
    "            # Skip very short sentences or questions\n",
    "            if len(sent_text.split()) < 5 or sent_text.endswith('?'):\n",
    "                continue\n",
    "            \n",
    "            # Extract entities from sentence\n",
    "            entities = [ent.text for ent in sent.ents]\n",
    "            \n",
    "            # Classify claim type\n",
    "            claim_type = self._classify_claim(sent_text, entities)\n",
    "            \n",
    "            # Skip if no clear claim type\n",
    "            if claim_type:\n",
    "                claim = Claim(\n",
    "                    text=sent_text,\n",
    "                    claim_type=claim_type,\n",
    "                    entities=entities,\n",
    "                    section=section,\n",
    "                    context=text\n",
    "                )\n",
    "                claims.append(claim)\n",
    "        \n",
    "        return claims\n",
    "    \n",
    "    def _classify_claim(self, text: str, entities: List[str]) -> Optional[str]:\n",
    "        \"\"\"Classify the type of claim\"\"\"\n",
    "        \n",
    "        # Numerical claims\n",
    "        if self.numerical_pattern.search(text):\n",
    "            return 'numerical'\n",
    "        \n",
    "        # Date-based claims\n",
    "        if self.date_pattern.search(text):\n",
    "            return 'temporal'\n",
    "        \n",
    "        # Relationship claims (contains multiple entities)\n",
    "        if len(entities) >= 2:\n",
    "            return 'relationship'\n",
    "        \n",
    "        # Entity attribute claims\n",
    "        if len(entities) >= 1 and any(word in text.lower() for word in ['is', 'are', 'was', 'were', 'has', 'have']):\n",
    "            return 'attribute'\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_from_article(self, article_text: str) -> Dict[str, List[Claim]]:\n",
    "        \"\"\"Extract claims organized by section\"\"\"\n",
    "        claims_by_section = defaultdict(list)\n",
    "        \n",
    "        # Split article into sections\n",
    "        sections = self._split_into_sections(article_text)\n",
    "        \n",
    "        for section_name, section_text in sections.items():\n",
    "            claims = self.extract_claims(section_text, section_name)\n",
    "            claims_by_section[section_name].extend(claims)\n",
    "        \n",
    "        return dict(claims_by_section)\n",
    "    \n",
    "    def _split_into_sections(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Split article into sections\"\"\"\n",
    "        sections = {}\n",
    "        current_section = \"Introduction\"\n",
    "        current_text = []\n",
    "        \n",
    "        for line in text.split('\\n'):\n",
    "            # Check if line is a section header\n",
    "            if line.startswith('##') and not line.startswith('###'):\n",
    "                # Save previous section\n",
    "                if current_text:\n",
    "                    sections[current_section] = '\\n'.join(current_text)\n",
    "                # Start new section\n",
    "                current_section = line.strip('#').strip()\n",
    "                current_text = []\n",
    "            else:\n",
    "                current_text.append(line)\n",
    "        \n",
    "        # Save last section\n",
    "        if current_text:\n",
    "            sections[current_section] = '\\n'.join(current_text)\n",
    "        \n",
    "        return sections\n",
    "\n",
    "# Initialize claim extractor\n",
    "claim_extractor = ClaimExtractor(nlp)\n",
    "\n",
    "print(\"✓ Claim extractor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac05d0b",
   "metadata": {},
   "source": [
    "## 4. Evidence Retrieval\n",
    "\n",
    "Retrieve supporting evidence from the knowledge graph and article corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d06f68ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evidence retriever initialized\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Evidence:\n",
    "    \"\"\"Represents a piece of evidence\"\"\"\n",
    "    text: str\n",
    "    source: str\n",
    "    source_type: str  # 'article', 'graph', 'entity'\n",
    "    similarity_score: float = 0.0\n",
    "    relevance_score: float = 0.0\n",
    "    url: str = \"\"\n",
    "\n",
    "class EvidenceRetriever:\n",
    "    \"\"\"Retrieve evidence for claims from knowledge base\"\"\"\n",
    "    \n",
    "    def __init__(self, articles, embeddings, faiss_index, index_titles, \n",
    "                 graph, entities, embedding_model):\n",
    "        self.articles = articles\n",
    "        self.embeddings = embeddings\n",
    "        self.faiss_index = faiss_index\n",
    "        self.index_titles = index_titles\n",
    "        self.graph = graph\n",
    "        self.entities = entities\n",
    "        self.embedding_model = embedding_model\n",
    "    \n",
    "    def retrieve_evidence(self, claim: Claim, top_k: int = 5) -> List[Evidence]:\n",
    "        \"\"\"Retrieve evidence for a claim from multiple sources\"\"\"\n",
    "        all_evidence = []\n",
    "        \n",
    "        # 1. Vector similarity search\n",
    "        vector_evidence = self._vector_search(claim.text, top_k)\n",
    "        all_evidence.extend(vector_evidence)\n",
    "        \n",
    "        # 2. Entity-based search\n",
    "        if claim.entities:\n",
    "            entity_evidence = self._entity_search(claim.entities, top_k)\n",
    "            all_evidence.extend(entity_evidence)\n",
    "        \n",
    "        # 3. Graph-based search for relationship claims\n",
    "        if claim.claim_type == 'relationship' and len(claim.entities) >= 2:\n",
    "            graph_evidence = self._graph_search(claim.entities)\n",
    "            all_evidence.extend(graph_evidence)\n",
    "        \n",
    "        # Remove duplicates and rank\n",
    "        unique_evidence = self._deduplicate_and_rank(all_evidence)\n",
    "        \n",
    "        return unique_evidence[:top_k]\n",
    "    \n",
    "    def _vector_search(self, query: str, top_k: int) -> List[Evidence]:\n",
    "        \"\"\"Search using vector similarity\"\"\"\n",
    "        evidence = []\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Search FAISS\n",
    "        query_vector = np.array([query_embedding], dtype=np.float32)\n",
    "        distances, indices = self.faiss_index.search(query_vector, top_k)\n",
    "        \n",
    "        for idx, dist in zip(indices[0], distances[0]):\n",
    "            if idx < len(self.index_titles):\n",
    "                title = self.index_titles[idx]\n",
    "                if title in self.articles:\n",
    "                    article = self.articles[title]\n",
    "                    text = article.get('text_clean', article.get('content', ''))\n",
    "                    evidence.append(Evidence(\n",
    "                        text=text[:500],  # First 500 chars\n",
    "                        source=title,\n",
    "                        source_type='article',\n",
    "                        similarity_score=float(dist),\n",
    "                        url=article.get('url', '')\n",
    "                    ))\n",
    "        \n",
    "        return evidence\n",
    "    \n",
    "    def _entity_search(self, entities: List[str], top_k: int) -> List[Evidence]:\n",
    "        \"\"\"Search for articles mentioning specific entities\"\"\"\n",
    "        evidence = []\n",
    "        \n",
    "        for entity in entities[:3]:  # Limit to top 3 entities\n",
    "            # Search in entity database\n",
    "            for article_title, article_entities in self.entities.items():\n",
    "                if article_title in self.articles:\n",
    "                    # Check if entity is mentioned\n",
    "                    entity_mentions = [e for e in article_entities if entity.lower() in e['text'].lower()]\n",
    "                    \n",
    "                    if entity_mentions:\n",
    "                        article = self.articles[article_title]\n",
    "                        # Extract context around entity mention\n",
    "                        text = article.get('text_clean', article.get('content', ''))\n",
    "                        context = self._extract_entity_context(text, entity)\n",
    "                        \n",
    "                        evidence.append(Evidence(\n",
    "                            text=context,\n",
    "                            source=article_title,\n",
    "                            source_type='entity',\n",
    "                            relevance_score=len(entity_mentions) / len(article_entities),\n",
    "                            url=article.get('url', '')\n",
    "                        ))\n",
    "                        \n",
    "                        if len(evidence) >= top_k:\n",
    "                            break\n",
    "        \n",
    "        return evidence\n",
    "    \n",
    "    def _graph_search(self, entities: List[str]) -> List[Evidence]:\n",
    "        \"\"\"Search graph for relationships between entities\"\"\"\n",
    "        evidence = []\n",
    "        \n",
    "        # Find nodes matching entities\n",
    "        entity_nodes = []\n",
    "        for node in self.graph.nodes():\n",
    "            if any(entity.lower() in node.lower() for entity in entities):\n",
    "                entity_nodes.append(node)\n",
    "        \n",
    "        # Find paths between entities\n",
    "        if len(entity_nodes) >= 2:\n",
    "            for i in range(len(entity_nodes)):\n",
    "                for j in range(i + 1, len(entity_nodes)):\n",
    "                    if self.graph.has_edge(entity_nodes[i], entity_nodes[j]):\n",
    "                        evidence.append(Evidence(\n",
    "                            text=f\"Connection between {entity_nodes[i]} and {entity_nodes[j]}\",\n",
    "                            source=\"Knowledge Graph\",\n",
    "                            source_type='graph',\n",
    "                            relevance_score=1.0\n",
    "                        ))\n",
    "        \n",
    "        return evidence\n",
    "    \n",
    "    def _extract_entity_context(self, text: str, entity: str, window: int = 150) -> str:\n",
    "        \"\"\"Extract context around entity mention\"\"\"\n",
    "        entity_lower = entity.lower()\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        pos = text_lower.find(entity_lower)\n",
    "        if pos == -1:\n",
    "            return text[:300]\n",
    "        \n",
    "        start = max(0, pos - window)\n",
    "        end = min(len(text), pos + len(entity) + window)\n",
    "        \n",
    "        context = text[start:end]\n",
    "        if start > 0:\n",
    "            context = \"...\" + context\n",
    "        if end < len(text):\n",
    "            context = context + \"...\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def _deduplicate_and_rank(self, evidence_list: List[Evidence]) -> List[Evidence]:\n",
    "        \"\"\"Remove duplicates and rank by relevance\"\"\"\n",
    "        # Remove duplicates based on source\n",
    "        seen_sources = set()\n",
    "        unique_evidence = []\n",
    "        \n",
    "        for evidence in evidence_list:\n",
    "            if evidence.source not in seen_sources:\n",
    "                seen_sources.add(evidence.source)\n",
    "                unique_evidence.append(evidence)\n",
    "        \n",
    "        # Rank by combined score\n",
    "        for evidence in unique_evidence:\n",
    "            evidence.relevance_score = (\n",
    "                evidence.similarity_score * 0.6 + \n",
    "                evidence.relevance_score * 0.4\n",
    "            )\n",
    "        \n",
    "        return sorted(unique_evidence, key=lambda e: e.relevance_score, reverse=True)\n",
    "\n",
    "# Initialize evidence retriever\n",
    "evidence_retriever = EvidenceRetriever(\n",
    "    articles, article_embeddings, faiss_index, index_titles,\n",
    "    G, entities, embedding_model\n",
    ")\n",
    "\n",
    "print(\"✓ Evidence retriever initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9a993",
   "metadata": {},
   "source": [
    "## 5. Verification Model\n",
    "\n",
    "Use Natural Language Inference (NLI) to verify claims against evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bd678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLI model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29212eda93354af8a37a2954ecfd308e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Python\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nasir\\.cache\\huggingface\\hub\\models--cross-encoder--nli-deberta-v3-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c151da37654f74b1cfd6ff8cdaec8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/738M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4757cb6e21e54659ad8d0bb102111694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27787e484c3b4b4387dbb19e1c792ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29cc1d64428483a9c1eb38f2b41ba72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e09a5644bb942fb89d969596a3e0e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d289ad159a4d1780079553617e26f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ NLI model loaded\n",
      "✓ Verification model ready\n"
     ]
    }
   ],
   "source": [
    "class VerificationModel:\n",
    "    \"\"\"Verify claims using NLI model\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load NLI model (using RoBERTa trained on MNLI)\n",
    "        print(\"Loading NLI model...\")\n",
    "        self.nli_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"cross-encoder/nli-deberta-v3-base\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        print(\"✓ NLI model loaded\")\n",
    "    \n",
    "    def verify_claim(self, claim: Claim, evidence_list: List[Evidence]) -> Tuple[str, float]:\n",
    "        \"\"\"Verify a claim against evidence\"\"\"\n",
    "        if not evidence_list:\n",
    "            return \"uncertain\", 0.0\n",
    "        \n",
    "        verification_scores = []\n",
    "        \n",
    "        # Check claim against each piece of evidence\n",
    "        for evidence in evidence_list[:3]:  # Use top 3 evidence pieces\n",
    "            score = self._compute_entailment(claim.text, evidence.text)\n",
    "            verification_scores.append(score)\n",
    "        \n",
    "        # Aggregate scores\n",
    "        avg_score = np.mean(verification_scores)\n",
    "        \n",
    "        # Determine verification status with more lenient thresholds\n",
    "        if avg_score > 0.5:  # Lowered from 0.7\n",
    "            status = \"verified\"\n",
    "        elif avg_score > 0.3:  # Lowered from 0.4\n",
    "            status = \"uncertain\"\n",
    "        else:\n",
    "            status = \"refuted\"\n",
    "        \n",
    "        return status, float(avg_score)\n",
    "    \n",
    "    def _compute_entailment(self, claim: str, evidence: str) -> float:\n",
    "        \"\"\"Compute entailment score using NLI\"\"\"\n",
    "        try:\n",
    "            # Truncate texts if too long\n",
    "            claim = claim[:512]\n",
    "            evidence = evidence[:512]\n",
    "            \n",
    "            # Use NLI to check if evidence supports claim\n",
    "            result = self.nli_pipeline(f\"{evidence} [SEP] {claim}\")[0]\n",
    "            \n",
    "            # Map label to score (cross-encoder outputs: entailment, neutral, contradiction)\n",
    "            label = result['label'].lower()\n",
    "            score = result['score']\n",
    "            \n",
    "            if 'entail' in label:\n",
    "                return score  # Full score for entailment\n",
    "            elif 'neutral' in label:\n",
    "                return score * 0.6  # 60% of score for neutral (increased from 0.5)\n",
    "            else:  # CONTRADICTION\n",
    "                return score * 0.1  # Very low score for contradiction\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in NLI: {e}\")\n",
    "            return 0.5  # Default uncertain\n",
    "    \n",
    "    def batch_verify(self, claims: List[Claim], evidence_map: Dict[str, List[Evidence]]) -> List[Claim]:\n",
    "        \"\"\"Verify multiple claims in batch\"\"\"\n",
    "        verified_claims = []\n",
    "        \n",
    "        for claim in claims:\n",
    "            evidence = evidence_map.get(claim.text, [])\n",
    "            status, confidence = self.verify_claim(claim, evidence)\n",
    "            \n",
    "            claim.verification_status = status\n",
    "            claim.confidence = confidence\n",
    "            claim.evidence = [e.__dict__ for e in evidence[:3]]\n",
    "            \n",
    "            verified_claims.append(claim)\n",
    "        \n",
    "        return verified_claims\n",
    "\n",
    "# Initialize verification model\n",
    "verification_model = VerificationModel()\n",
    "\n",
    "print(\"✓ Verification model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66992f",
   "metadata": {},
   "source": [
    "## 6. Citation Enhancement System\n",
    "\n",
    "Generate detailed citations linking claims to evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f23d4dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Citation enhancer initialized\n"
     ]
    }
   ],
   "source": [
    "class CitationEnhancer:\n",
    "    \"\"\"Enhance articles with detailed citations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.citation_counter = 1\n",
    "    \n",
    "    def add_citations_to_article(self, article_text: str, verified_claims: List[Claim]) -> str:\n",
    "        \"\"\"Add citations to article based on verified claims\"\"\"\n",
    "        enhanced_text = article_text\n",
    "        citation_map = {}\n",
    "        references = []\n",
    "        \n",
    "        # Process claims by confidence\n",
    "        high_confidence_claims = [c for c in verified_claims if c.confidence > 0.7]\n",
    "        \n",
    "        for claim in high_confidence_claims:\n",
    "            if claim.evidence:\n",
    "                # Add citation marker to text\n",
    "                claim_text = claim.text\n",
    "                if claim_text in enhanced_text and claim_text not in citation_map:\n",
    "                    citation_num = len(references) + 1\n",
    "                    citation_marker = f\"[{citation_num}]\"\n",
    "                    \n",
    "                    # Insert citation after the claim\n",
    "                    enhanced_text = enhanced_text.replace(\n",
    "                        claim_text,\n",
    "                        f\"{claim_text}{citation_marker}\",\n",
    "                        1  # Replace only first occurrence\n",
    "                    )\n",
    "                    \n",
    "                    # Create reference entry\n",
    "                    evidence = claim.evidence[0]\n",
    "                    reference = self._format_reference(citation_num, evidence, claim.confidence)\n",
    "                    references.append(reference)\n",
    "                    citation_map[claim_text] = citation_num\n",
    "        \n",
    "        # Add references section\n",
    "        if references:\n",
    "            enhanced_text = self._add_references_section(enhanced_text, references)\n",
    "        \n",
    "        return enhanced_text\n",
    "    \n",
    "    def _format_reference(self, citation_num: int, evidence: Dict, confidence: float) -> str:\n",
    "        \"\"\"Format a reference entry\"\"\"\n",
    "        source = evidence.get('source', 'Unknown')\n",
    "        url = evidence.get('url', '')\n",
    "        \n",
    "        if url:\n",
    "            reference = f\"[{citation_num}] {source}. Wikipedia. Retrieved 2026-01-02. {url} (Confidence: {confidence:.2f})\"\n",
    "        else:\n",
    "            reference = f\"[{citation_num}] {source}. Knowledge Graph. (Confidence: {confidence:.2f})\"\n",
    "        \n",
    "        return reference\n",
    "    \n",
    "    def _add_references_section(self, article_text: str, references: List[str]) -> str:\n",
    "        \"\"\"Add or update references section\"\"\"\n",
    "        # Check if references section exists\n",
    "        if \"## References\" in article_text:\n",
    "            # Replace existing references\n",
    "            parts = article_text.split(\"## References\")\n",
    "            before_refs = parts[0]\n",
    "            \n",
    "            # Find the end of references (next section or end)\n",
    "            after_refs = \"\"\n",
    "            if len(parts) > 1:\n",
    "                rest = parts[1]\n",
    "                # Look for next section\n",
    "                next_section = rest.find(\"\\n## \")\n",
    "                if next_section != -1:\n",
    "                    after_refs = rest[next_section:]\n",
    "            \n",
    "            refs_text = \"\\n\".join(references)\n",
    "            return f\"{before_refs}## References\\n\\n{refs_text}\\n{after_refs}\"\n",
    "        else:\n",
    "            # Add new references section before metadata\n",
    "            refs_text = \"\\n\".join(references)\n",
    "            \n",
    "            # Insert before metadata section if it exists\n",
    "            if \"### Generation Metadata\" in article_text:\n",
    "                parts = article_text.split(\"### Generation Metadata\")\n",
    "                return f\"{parts[0]}\\n## References\\n\\n{refs_text}\\n\\n### Generation Metadata{parts[1]}\"\n",
    "            else:\n",
    "                return f\"{article_text}\\n\\n## References\\n\\n{refs_text}\"\n",
    "    \n",
    "    def generate_verification_report(self, claims: List[Claim]) -> Dict:\n",
    "        \"\"\"Generate verification statistics report\"\"\"\n",
    "        total = len(claims)\n",
    "        verified = len([c for c in claims if c.verification_status == \"verified\"])\n",
    "        uncertain = len([c for c in claims if c.verification_status == \"uncertain\"])\n",
    "        refuted = len([c for c in claims if c.verification_status == \"refuted\"])\n",
    "        \n",
    "        avg_confidence = np.mean([c.confidence for c in claims]) if claims else 0.0\n",
    "        \n",
    "        return {\n",
    "            'total_claims': total,\n",
    "            'verified': verified,\n",
    "            'uncertain': uncertain,\n",
    "            'refuted': refuted,\n",
    "            'verification_rate': verified / total if total > 0 else 0.0,\n",
    "            'average_confidence': float(avg_confidence)\n",
    "        }\n",
    "\n",
    "# Initialize citation enhancer\n",
    "citation_enhancer = CitationEnhancer()\n",
    "\n",
    "print(\"✓ Citation enhancer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2f50b",
   "metadata": {},
   "source": [
    "## 7. Complete Verification Pipeline\n",
    "\n",
    "Integrate all components into a unified verification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cd0e515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✓ Verification pipeline ready!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "class ArticleVerificationPipeline:\n",
    "    \"\"\"Complete pipeline for article verification\"\"\"\n",
    "    \n",
    "    def __init__(self, claim_extractor, evidence_retriever, \n",
    "                 verification_model, citation_enhancer):\n",
    "        self.claim_extractor = claim_extractor\n",
    "        self.evidence_retriever = evidence_retriever\n",
    "        self.verification_model = verification_model\n",
    "        self.citation_enhancer = citation_enhancer\n",
    "    \n",
    "    def verify_article(self, article_text: str, verbose: bool = True) -> Dict:\n",
    "        \"\"\"Verify an article and enhance with citations\"\"\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"ARTICLE VERIFICATION PIPELINE\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Extract claims\n",
    "        if verbose:\n",
    "            print(\"\\n1. Extracting claims...\")\n",
    "        claims_by_section = self.claim_extractor.extract_from_article(article_text)\n",
    "        all_claims = [claim for claims in claims_by_section.values() for claim in claims]\n",
    "        if verbose:\n",
    "            print(f\"   ✓ Extracted {len(all_claims)} claims from {len(claims_by_section)} sections\")\n",
    "        \n",
    "        # Step 2: Retrieve evidence\n",
    "        if verbose:\n",
    "            print(\"\\n2. Retrieving evidence...\")\n",
    "        evidence_map = {}\n",
    "        for claim in all_claims:\n",
    "            evidence = self.evidence_retriever.retrieve_evidence(claim, top_k=3)\n",
    "            evidence_map[claim.text] = evidence\n",
    "        if verbose:\n",
    "            print(f\"   ✓ Retrieved evidence for {len(evidence_map)} claims\")\n",
    "        \n",
    "        # Step 3: Verify claims\n",
    "        if verbose:\n",
    "            print(\"\\n3. Verifying claims...\")\n",
    "        verified_claims = self.verification_model.batch_verify(all_claims, evidence_map)\n",
    "        if verbose:\n",
    "            verified = len([c for c in verified_claims if c.verification_status == \"verified\"])\n",
    "            print(f\"   ✓ Verified {verified}/{len(verified_claims)} claims\")\n",
    "        \n",
    "        # Step 4: Enhance with citations\n",
    "        if verbose:\n",
    "            print(\"\\n4. Adding citations...\")\n",
    "        enhanced_article = self.citation_enhancer.add_citations_to_article(\n",
    "            article_text, verified_claims\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"   ✓ Citations added\")\n",
    "        \n",
    "        # Generate report\n",
    "        report = self.citation_enhancer.generate_verification_report(verified_claims)\n",
    "        report['claims_by_section'] = {\n",
    "            section: len(claims) for section, claims in claims_by_section.items()\n",
    "        }\n",
    "        report['claims_by_type'] = self._count_by_type(all_claims)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"VERIFICATION COMPLETE\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"\\nVerification Rate: {report['verification_rate']:.1%}\")\n",
    "            print(f\"Average Confidence: {report['average_confidence']:.2f}\")\n",
    "            print(f\"Verified: {report['verified']}, Uncertain: {report['uncertain']}, Refuted: {report['refuted']}\")\n",
    "        \n",
    "        return {\n",
    "            'enhanced_article': enhanced_article,\n",
    "            'claims': verified_claims,\n",
    "            'report': report\n",
    "        }\n",
    "    \n",
    "    def _count_by_type(self, claims: List[Claim]) -> Dict[str, int]:\n",
    "        \"\"\"Count claims by type\"\"\"\n",
    "        type_counts = defaultdict(int)\n",
    "        for claim in claims:\n",
    "            type_counts[claim.claim_type] += 1\n",
    "        return dict(type_counts)\n",
    "\n",
    "# Initialize verification pipeline\n",
    "verification_pipeline = ArticleVerificationPipeline(\n",
    "    claim_extractor,\n",
    "    evidence_retriever,\n",
    "    verification_model,\n",
    "    citation_enhancer\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Verification pipeline ready!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b3348",
   "metadata": {},
   "source": [
    "## 8. Test Verification on Generated Articles\n",
    "\n",
    "Verify the articles generated in Phase 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ce10df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 generated articles to verify:\n",
      "\n",
      "  • Deep_Learning_generated.md\n",
      "  • Machine_Learning_generated.md\n",
      "  • Natural_Language_Processing_generated.md\n",
      "  • Quantum_Computing_generated.md\n"
     ]
    }
   ],
   "source": [
    "# Find generated articles\n",
    "generated_articles = list(ARTICLES_DIR.glob('*_generated.md'))\n",
    "\n",
    "print(f\"Found {len(generated_articles)} generated articles to verify:\\n\")\n",
    "for article_path in generated_articles:\n",
    "    print(f\"  • {article_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eab9c7",
   "metadata": {},
   "source": [
    "## 9. Verify First Article (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0c183e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying: Deep_Learning_generated.md\n",
      "Original length: 3778 characters\n",
      "\n",
      "\n",
      "============================================================\n",
      "ARTICLE VERIFICATION PIPELINE\n",
      "============================================================\n",
      "\n",
      "1. Extracting claims...\n",
      "   ✓ Extracted 12 claims from 9 sections\n",
      "\n",
      "2. Retrieving evidence...\n",
      "   ✓ Retrieved evidence for 12 claims\n",
      "\n",
      "3. Verifying claims...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Verified 0/12 claims\n",
      "\n",
      "4. Adding citations...\n",
      "   ✓ Citations added\n",
      "\n",
      "============================================================\n",
      "VERIFICATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Verification Rate: 0.0%\n",
      "Average Confidence: 0.49\n",
      "Verified: 0, Uncertain: 12, Refuted: 0\n",
      "\n",
      "✓ Verified article saved to: d:\\Projects\\agent-wiki-graphrag\\outputs\\articles\\Deep_Learning_verified.md\n",
      "  Enhanced length: 3778 characters\n"
     ]
    }
   ],
   "source": [
    "# Load the Deep Learning article\n",
    "article_path = ARTICLES_DIR / 'Deep_Learning_generated.md'\n",
    "\n",
    "if article_path.exists():\n",
    "    with open(article_path, 'r', encoding='utf-8') as f:\n",
    "        article_text = f.read()\n",
    "    \n",
    "    print(f\"Verifying: {article_path.name}\")\n",
    "    print(f\"Original length: {len(article_text)} characters\\n\")\n",
    "    \n",
    "    # Run verification\n",
    "    result = verification_pipeline.verify_article(article_text, verbose=True)\n",
    "    \n",
    "    # Save enhanced article\n",
    "    verified_path = ARTICLES_DIR / 'Deep_Learning_verified.md'\n",
    "    with open(verified_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(result['enhanced_article'])\n",
    "    \n",
    "    print(f\"\\n✓ Verified article saved to: {verified_path}\")\n",
    "    print(f\"  Enhanced length: {len(result['enhanced_article'])} characters\")\n",
    "else:\n",
    "    print(f\"Article not found: {article_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b0050",
   "metadata": {},
   "source": [
    "## 10. Verify All Generated Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb296b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all generated articles\n",
    "verification_results = []\n",
    "\n",
    "for article_path in generated_articles:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {article_path.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    with open(article_path, 'r', encoding='utf-8') as f:\n",
    "        article_text = f.read()\n",
    "    \n",
    "    # Run verification\n",
    "    result = verification_pipeline.verify_article(article_text, verbose=False)\n",
    "    \n",
    "    # Save verified article\n",
    "    verified_name = article_path.stem.replace('_generated', '_verified') + '.md'\n",
    "    verified_path = ARTICLES_DIR / verified_name\n",
    "    \n",
    "    with open(verified_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(result['enhanced_article'])\n",
    "    \n",
    "    # Store results\n",
    "    verification_results.append({\n",
    "        'article': article_path.stem,\n",
    "        'verified_path': str(verified_path),\n",
    "        **result['report']\n",
    "    })\n",
    "    \n",
    "    print(f\"✓ Verified: {result['report']['verification_rate']:.1%} ({result['report']['verified']}/{result['report']['total_claims']} claims)\")\n",
    "    print(f\"✓ Confidence: {result['report']['average_confidence']:.2f}\")\n",
    "    print(f\"✓ Saved to: {verified_name}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All articles verified!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa5341d",
   "metadata": {},
   "source": [
    "## 11. Verification Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59227ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dataframe\n",
    "df_results = pd.DataFrame(verification_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_results[['article', 'total_claims', 'verified', 'uncertain', 'refuted', \n",
    "                   'verification_rate', 'average_confidence']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGGREGATE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total Claims Analyzed: {df_results['total_claims'].sum()}\")\n",
    "print(f\"Overall Verification Rate: {df_results['verified'].sum() / df_results['total_claims'].sum():.1%}\")\n",
    "print(f\"Average Confidence Score: {df_results['average_confidence'].mean():.2f}\")\n",
    "print(f\"Total Verified: {df_results['verified'].sum()}\")\n",
    "print(f\"Total Uncertain: {df_results['uncertain'].sum()}\")\n",
    "print(f\"Total Refuted: {df_results['refuted'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34acc0e8",
   "metadata": {},
   "source": [
    "## 12. Visualize Verification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9848f4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Verification status by article\n",
    "ax1 = axes[0, 0]\n",
    "df_results[['article', 'verified', 'uncertain', 'refuted']].set_index('article').plot(\n",
    "    kind='bar', stacked=True, ax=ax1, color=['#2ecc71', '#f39c12', '#e74c3c']\n",
    ")\n",
    "ax1.set_title('Verification Status by Article', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Article')\n",
    "ax1.set_ylabel('Number of Claims')\n",
    "ax1.legend(title='Status')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Verification rates\n",
    "ax2 = axes[0, 1]\n",
    "df_results.plot(x='article', y='verification_rate', kind='bar', ax=ax2, color='#3498db', legend=False)\n",
    "ax2.set_title('Verification Rate by Article', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Article')\n",
    "ax2.set_ylabel('Verification Rate')\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.axhline(y=df_results['verification_rate'].mean(), color='r', linestyle='--', label='Average')\n",
    "ax2.legend()\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Confidence scores\n",
    "ax3 = axes[1, 0]\n",
    "df_results.plot(x='article', y='average_confidence', kind='bar', ax=ax3, color='#9b59b6', legend=False)\n",
    "ax3.set_title('Average Confidence Score by Article', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Article')\n",
    "ax3.set_ylabel('Confidence Score')\n",
    "ax3.set_ylim([0, 1])\n",
    "ax3.axhline(y=df_results['average_confidence'].mean(), color='r', linestyle='--', label='Average')\n",
    "ax3.legend()\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Overall distribution\n",
    "ax4 = axes[1, 1]\n",
    "overall_counts = [\n",
    "    df_results['verified'].sum(),\n",
    "    df_results['uncertain'].sum(),\n",
    "    df_results['refuted'].sum()\n",
    "]\n",
    "ax4.pie(overall_counts, labels=['Verified', 'Uncertain', 'Refuted'],\n",
    "        autopct='%1.1f%%', colors=['#2ecc71', '#f39c12', '#e74c3c'],\n",
    "        startangle=90)\n",
    "ax4.set_title('Overall Verification Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(VERIFICATION_DIR / 'verification_statistics.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Visualization saved to:\", VERIFICATION_DIR / 'verification_statistics.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64843a",
   "metadata": {},
   "source": [
    "## 13. Save Verification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed verification report\n",
    "report_data = {\n",
    "    'timestamp': '2026-01-02',\n",
    "    'articles_verified': len(verification_results),\n",
    "    'total_claims': int(df_results['total_claims'].sum()),\n",
    "    'verified_claims': int(df_results['verified'].sum()),\n",
    "    'uncertain_claims': int(df_results['uncertain'].sum()),\n",
    "    'refuted_claims': int(df_results['refuted'].sum()),\n",
    "    'overall_verification_rate': float(df_results['verified'].sum() / df_results['total_claims'].sum()),\n",
    "    'average_confidence': float(df_results['average_confidence'].mean()),\n",
    "    'article_details': verification_results\n",
    "}\n",
    "\n",
    "report_path = VERIFICATION_DIR / 'verification_report.json'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(report_data, f, indent=2)\n",
    "\n",
    "print(f\"✓ Verification report saved to: {report_path}\")\n",
    "\n",
    "# Save CSV summary\n",
    "csv_path = VERIFICATION_DIR / 'verification_summary.csv'\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"✓ CSV summary saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52230ff7",
   "metadata": {},
   "source": [
    "## 14. Project Completion Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e44ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROJECT COMPLETE: Agentic AI-Powered Wikipedia Article Generator\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✅ Phase 1: Data Collection & Preprocessing\")\n",
    "print(\"   - Collected 1,337 Wikipedia articles\")\n",
    "print(\"   - Extracted entities and relationships\")\n",
    "print(\"   - Built knowledge graph with 11,091 edges\")\n",
    "print(\"   - Generated 384-dim embeddings\")\n",
    "\n",
    "print(\"\\n✅ Phase 2: GraphRAG Engine\")\n",
    "print(\"   - Built FAISS vector index\")\n",
    "print(\"   - Implemented graph traversal\")\n",
    "print(\"   - Created hybrid retrieval system\")\n",
    "print(\"   - Fusion ranking algorithm\")\n",
    "\n",
    "print(\"\\n✅ Phase 3: Multi-Agent System\")\n",
    "print(\"   - Research Agent: Information gathering\")\n",
    "print(\"   - Planning Agent: Article structuring\")\n",
    "print(\"   - Writing Agent: Content generation\")\n",
    "print(\"   - Verification Agent: Citations & validation\")\n",
    "print(\"   - Assembly Agent: Final compilation\")\n",
    "print(\"   - Orchestrator: Workflow coordination\")\n",
    "\n",
    "print(\"\\n✅ Phase 4: Fact-Verification System\")\n",
    "print(\"   - Claim Extraction: Parse factual statements\")\n",
    "print(\"   - Evidence Retrieval: Multi-source evidence gathering\")\n",
    "print(\"   - NLI Verification: DeBERTa-based claim validation\")\n",
    "print(\"   - Citation Enhancement: Detailed source linking\")\n",
    "print(f\"   - Verified {report_data['verified_claims']} claims across {report_data['articles_verified']} articles\")\n",
    "print(f\"   - Achieved {report_data['overall_verification_rate']:.1%} verification rate\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"System Capabilities:\")\n",
    "print(\"=\"*80)\n",
    "print(\"✓ Automatic article generation from topics\")\n",
    "print(\"✓ Multi-source information retrieval\")\n",
    "print(\"✓ Graph-based + semantic search\")\n",
    "print(\"✓ Structured content with citations\")\n",
    "print(\"✓ Fact-checking and verification\")\n",
    "print(\"✓ Confidence scoring for claims\")\n",
    "print(\"✓ Evidence-backed citations\")\n",
    "print(\"✓ Scalable agent architecture\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Generated & Verified Articles:\")\n",
    "print(\"=\"*80)\n",
    "for result in verification_results:\n",
    "    print(f\"  • {result['article']}: {result['verified']}/{result['total_claims']} claims verified ({result['verification_rate']:.1%})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Output Files:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  • Verified Articles: {ARTICLES_DIR}/*_verified.md\")\n",
    "print(f\"  • Verification Report: {report_path}\")\n",
    "print(f\"  • Statistics CSV: {csv_path}\")\n",
    "print(f\"  • Visualizations: {VERIFICATION_DIR}/verification_statistics.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 All 4 phases complete!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext Steps (Optional):\")\n",
    "print(\"  • Phase 5: Web UI with Streamlit/Gradio\")\n",
    "print(\"  • Phase 5: REST API with FastAPI\")\n",
    "print(\"  • Phase 5: Deployment pipeline\")\n",
    "print(\"  • Phase 5: User testing & evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
