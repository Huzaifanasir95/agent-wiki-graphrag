{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1261c31",
   "metadata": {},
   "source": [
    "# Phase 3: Multi-Agent System\n",
    "## Agentic AI for Wikipedia Article Generation\n",
    "\n",
    "**Objective**: Build a multi-agent system that collaborates to generate high-quality Wikipedia-style articles.\n",
    "\n",
    "**Agents we'll implement:**\n",
    "1. **Research Agent** - Gathers information using GraphRAG\n",
    "2. **Planning Agent** - Creates article outline and structure\n",
    "3. **Writing Agent** - Generates coherent content sections\n",
    "4. **Fact-Verification Agent** - Validates claims (simplified version)\n",
    "5. **Orchestrator** - Coordinates the workflow\n",
    "\n",
    "**What we'll do in this notebook:**\n",
    "1. Load the GraphRAG engine from Phase 2\n",
    "2. Set up a simple agent framework (using classes)\n",
    "3. Implement each specialized agent\n",
    "4. Create an orchestration workflow\n",
    "5. Generate a complete Wikipedia article\n",
    "6. Evaluate the output\n",
    "\n",
    "**Note**: This is a simplified version without LLM API calls. We'll use templates and the GraphRAG system to demonstrate the architecture.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d54fc9a",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "Import required libraries for the agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838808e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Graph and vector search\n",
    "import networkx as nx\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9b90fd",
   "metadata": {},
   "source": [
    "## Step 2: Load GraphRAG Engine\n",
    "\n",
    "Load all components from Phase 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a0ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "PROJECT_ROOT = Path(r\"d:\\Projects\\agent-wiki-graphrag\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "EMBEDDINGS_DIR = DATA_DIR / \"embeddings\"\n",
    "KG_DIR = DATA_DIR / \"knowledge_graph\"\n",
    "OUTPUTS_DIR = PROJECT_ROOT / \"outputs\" / \"articles\"\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Loading GraphRAG engine components...\")\n",
    "\n",
    "# Load articles\n",
    "with open(RAW_DIR / \"wikipedia_articles.json\", 'r', encoding='utf-8') as f:\n",
    "    articles = json.load(f)\n",
    "print(f\"âœ“ Loaded {len(articles)} articles\")\n",
    "\n",
    "# Load knowledge graph\n",
    "with open(KG_DIR / \"article_graph.pkl\", 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "print(f\"âœ“ Loaded knowledge graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "\n",
    "# Load FAISS index\n",
    "faiss_index = faiss.read_index(str(EMBEDDINGS_DIR / \"faiss_index.bin\"))\n",
    "with open(EMBEDDINGS_DIR / \"index_titles.json\", 'r', encoding='utf-8') as f:\n",
    "    index_titles = json.load(f)\n",
    "print(f\"âœ“ Loaded FAISS index: {faiss_index.ntotal} vectors\")\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"âœ“ Loaded embedding model\")\n",
    "\n",
    "# Load entities\n",
    "with open(PROCESSED_DIR / \"entities.json\", 'r', encoding='utf-8') as f:\n",
    "    entities = json.load(f)\n",
    "print(f\"âœ“ Loaded entities for {len(entities)} articles\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GraphRAG Engine Ready!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f86c10",
   "metadata": {},
   "source": [
    "## Step 3: Rebuild Retriever Classes\n",
    "\n",
    "Reconstruct the retriever classes from Phase 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8195354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorRetriever:\n",
    "    \"\"\"Vector-based retrieval\"\"\"\n",
    "    def __init__(self, index, titles, encoder):\n",
    "        self.index = index\n",
    "        self.titles = titles\n",
    "        self.encoder = encoder\n",
    "    \n",
    "    def search(self, query, top_k=10):\n",
    "        query_vec = self.encoder.encode([query]).astype(np.float32)\n",
    "        query_vec = np.ascontiguousarray(query_vec)\n",
    "        faiss.normalize_L2(query_vec)\n",
    "        scores, indices = self.index.search(query_vec, top_k)\n",
    "        return [(self.titles[idx], float(scores[0][i])) \n",
    "                for i, idx in enumerate(indices[0]) if idx < len(self.titles)]\n",
    "\n",
    "class GraphRetriever:\n",
    "    \"\"\"Graph-based retrieval\"\"\"\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "    \n",
    "    def multi_hop_retrieval(self, start_nodes, max_hops=2, max_results=50):\n",
    "        if isinstance(start_nodes, str):\n",
    "            start_nodes = [start_nodes]\n",
    "        \n",
    "        visited = set()\n",
    "        results = []\n",
    "        queue = [(node, 0) for node in start_nodes if node in self.graph]\n",
    "        \n",
    "        while queue and len(results) < max_results:\n",
    "            current_node, current_hop = queue.pop(0)\n",
    "            if current_node in visited:\n",
    "                continue\n",
    "            visited.add(current_node)\n",
    "            results.append((current_node, current_hop))\n",
    "            \n",
    "            if current_hop < max_hops:\n",
    "                neighbors = list(self.graph.successors(current_node))[:10]\n",
    "                for neighbor in neighbors:\n",
    "                    if neighbor not in visited:\n",
    "                        queue.append((neighbor, current_hop + 1))\n",
    "        return results\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"Hybrid retrieval\"\"\"\n",
    "    def __init__(self, vector_retriever, graph_retriever, articles):\n",
    "        self.vector = vector_retriever\n",
    "        self.graph = graph_retriever\n",
    "        self.articles = articles\n",
    "    \n",
    "    def retrieve(self, query, top_k=10, alpha=0.5):\n",
    "        results = {}\n",
    "        \n",
    "        # Vector search\n",
    "        vector_results = self.vector.search(query, top_k=top_k*2)\n",
    "        for title, score in vector_results:\n",
    "            results[title] = {'vector_score': score, 'graph_score': 0}\n",
    "        \n",
    "        # Graph search\n",
    "        query_terms = query.lower().split()\n",
    "        matching_articles = [t for t in self.articles.keys() \n",
    "                           if any(term in t.lower() for term in query_terms)]\n",
    "        \n",
    "        if matching_articles:\n",
    "            graph_results = self.graph.multi_hop_retrieval(matching_articles[:3], \n",
    "                                                          max_hops=2, max_results=top_k*2)\n",
    "            for title, hop in graph_results:\n",
    "                if title not in results:\n",
    "                    results[title] = {'vector_score': 0, 'graph_score': 0}\n",
    "                results[title]['graph_score'] = 1.0 / (2 ** hop)\n",
    "        \n",
    "        # Normalize and rank\n",
    "        max_v = max((r['vector_score'] for r in results.values()), default=1.0)\n",
    "        max_g = max((r['graph_score'] for r in results.values()), default=1.0)\n",
    "        \n",
    "        ranked = []\n",
    "        for title, scores in results.items():\n",
    "            v_norm = scores['vector_score'] / max_v if max_v > 0 else 0\n",
    "            g_norm = scores['graph_score'] / max_g if max_g > 0 else 0\n",
    "            final_score = alpha * v_norm + (1 - alpha) * g_norm\n",
    "            ranked.append((title, final_score))\n",
    "        \n",
    "        ranked.sort(key=lambda x: x[1], reverse=True)\n",
    "        return ranked[:top_k]\n",
    "\n",
    "# Initialize retrievers\n",
    "vector_retriever = VectorRetriever(faiss_index, index_titles, embedding_model)\n",
    "graph_retriever = GraphRetriever(G)\n",
    "hybrid_retriever = HybridRetriever(vector_retriever, graph_retriever, articles)\n",
    "\n",
    "print(\"âœ“ Retrievers initialized and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b07198",
   "metadata": {},
   "source": [
    "## Step 4: Define Agent State & Message Types\n",
    "\n",
    "Create data structures for agent communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea90f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentMessage:\n",
    "    \"\"\"Message passed between agents\"\"\"\n",
    "    agent: str\n",
    "    content: Any\n",
    "    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class ArticleState:\n",
    "    \"\"\"State of the article generation process\"\"\"\n",
    "    topic: str\n",
    "    research_data: Dict = field(default_factory=dict)\n",
    "    outline: List[str] = field(default_factory=list)\n",
    "    sections: Dict[str, str] = field(default_factory=dict)\n",
    "    citations: List[Dict] = field(default_factory=list)\n",
    "    verification_results: Dict = field(default_factory=dict)\n",
    "    final_article: str = \"\"\n",
    "    messages: List[AgentMessage] = field(default_factory=list)\n",
    "    \n",
    "    def add_message(self, agent: str, content: Any, **metadata):\n",
    "        \"\"\"Add a message to the conversation\"\"\"\n",
    "        msg = AgentMessage(agent=agent, content=content, metadata=metadata)\n",
    "        self.messages.append(msg)\n",
    "        return msg\n",
    "\n",
    "print(\"âœ“ Agent state structures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0ec41",
   "metadata": {},
   "source": [
    "## Step 5: Implement Research Agent\n",
    "\n",
    "Agent that gathers information using GraphRAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAgent:\n",
    "    \"\"\"Agent responsible for gathering information\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever, articles):\n",
    "        self.retriever = retriever\n",
    "        self.articles = articles\n",
    "        self.name = \"Research Agent\"\n",
    "    \n",
    "    def research(self, state: ArticleState, top_k=10) -> ArticleState:\n",
    "        \"\"\"Gather information about the topic\"\"\"\n",
    "        print(f\"\\n{self.name}: Researching topic '{state.topic}'...\")\n",
    "        \n",
    "        # Retrieve relevant articles\n",
    "        results = self.retriever.retrieve(state.topic, top_k=top_k)\n",
    "        \n",
    "        # Gather context\n",
    "        research_data = {\n",
    "            'relevant_articles': [],\n",
    "            'key_entities': defaultdict(int),\n",
    "            'categories': defaultdict(int),\n",
    "            'total_sources': len(results)\n",
    "        }\n",
    "        \n",
    "        for title, score in results:\n",
    "            if title in self.articles:\n",
    "                article = self.articles[title]\n",
    "                \n",
    "                # Store article info\n",
    "                article_info = {\n",
    "                    'title': title,\n",
    "                    'score': score,\n",
    "                    'summary': article.get('summary_clean', '')[:500],\n",
    "                    'url': article.get('url', ''),\n",
    "                    'categories': article.get('categories', [])[:3]\n",
    "                }\n",
    "                research_data['relevant_articles'].append(article_info)\n",
    "                \n",
    "                # Aggregate entities\n",
    "                if title in entities:\n",
    "                    for ent in entities[title]:\n",
    "                        research_data['key_entities'][ent['text']] += 1\n",
    "                \n",
    "                # Aggregate categories\n",
    "                for cat in article.get('categories', []):\n",
    "                    research_data['categories'][cat] += 1\n",
    "        \n",
    "        # Get top entities and categories\n",
    "        research_data['key_entities'] = dict(\n",
    "            sorted(research_data['key_entities'].items(), \n",
    "                   key=lambda x: x[1], reverse=True)[:20]\n",
    "        )\n",
    "        research_data['categories'] = dict(\n",
    "            sorted(research_data['categories'].items(), \n",
    "                   key=lambda x: x[1], reverse=True)[:10]\n",
    "        )\n",
    "        \n",
    "        state.research_data = research_data\n",
    "        state.add_message(self.name, \n",
    "                         f\"Gathered {len(results)} relevant sources\",\n",
    "                         sources=len(results))\n",
    "        \n",
    "        print(f\"  âœ“ Found {len(results)} relevant articles\")\n",
    "        print(f\"  âœ“ Identified {len(research_data['key_entities'])} key entities\")\n",
    "        print(f\"  âœ“ Top categories: {list(research_data['categories'].keys())[:3]}\")\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Initialize research agent\n",
    "research_agent = ResearchAgent(hybrid_retriever, articles)\n",
    "print(\"âœ“ Research Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6861f953",
   "metadata": {},
   "source": [
    "## Step 6: Implement Planning Agent\n",
    "\n",
    "Agent that creates article structure and outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanningAgent:\n",
    "    \"\"\"Agent responsible for creating article outline\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Planning Agent\"\n",
    "    \n",
    "    def plan(self, state: ArticleState) -> ArticleState:\n",
    "        \"\"\"Create article outline based on research\"\"\"\n",
    "        print(f\"\\n{self.name}: Creating article outline...\")\n",
    "        \n",
    "        research = state.research_data\n",
    "        topic = state.topic\n",
    "        \n",
    "        # Basic Wikipedia article structure\n",
    "        outline = [\n",
    "            \"Introduction\",\n",
    "            \"Overview\",\n",
    "            \"History\",\n",
    "            \"Key Concepts\",\n",
    "            \"Applications\",\n",
    "            \"Challenges and Limitations\",\n",
    "            \"Future Directions\",\n",
    "            \"See Also\",\n",
    "            \"References\"\n",
    "        ]\n",
    "        \n",
    "        # Customize based on topic\n",
    "        if any(term in topic.lower() for term in ['machine learning', 'ai', 'algorithm']):\n",
    "            outline = [\n",
    "                \"Introduction\",\n",
    "                \"Definition and Overview\",\n",
    "                \"History and Development\",\n",
    "                \"How It Works\",\n",
    "                \"Types and Variants\",\n",
    "                \"Applications\",\n",
    "                \"Advantages and Disadvantages\",\n",
    "                \"Current Research\",\n",
    "                \"See Also\",\n",
    "                \"References\"\n",
    "            ]\n",
    "        elif any(term in topic.lower() for term in ['quantum', 'physics']):\n",
    "            outline = [\n",
    "                \"Introduction\",\n",
    "                \"Overview\",\n",
    "                \"Theoretical Background\",\n",
    "                \"Key Principles\",\n",
    "                \"Experimental Evidence\",\n",
    "                \"Applications\",\n",
    "                \"Interpretations\",\n",
    "                \"Current Research\",\n",
    "                \"See Also\",\n",
    "                \"References\"\n",
    "            ]\n",
    "        \n",
    "        state.outline = outline\n",
    "        state.add_message(self.name, \n",
    "                         f\"Created outline with {len(outline)} sections\",\n",
    "                         sections=len(outline))\n",
    "        \n",
    "        print(f\"  âœ“ Created outline with {len(outline)} sections:\")\n",
    "        for i, section in enumerate(outline[:5], 1):\n",
    "            print(f\"    {i}. {section}\")\n",
    "        if len(outline) > 5:\n",
    "            print(f\"    ... and {len(outline) - 5} more\")\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Initialize planning agent\n",
    "planning_agent = PlanningAgent()\n",
    "print(\"âœ“ Planning Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339a4c0",
   "metadata": {},
   "source": [
    "## Step 7: Implement Writing Agent\n",
    "\n",
    "Agent that generates content for each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WritingAgent:\n",
    "    \"\"\"Agent responsible for writing article content\"\"\"\n",
    "    \n",
    "    def __init__(self, articles):\n",
    "        self.articles = articles\n",
    "        self.name = \"Writing Agent\"\n",
    "    \n",
    "    def write(self, state: ArticleState) -> ArticleState:\n",
    "        \"\"\"Write content for each section\"\"\"\n",
    "        print(f\"\\n{self.name}: Writing article sections...\")\n",
    "        \n",
    "        topic = state.topic\n",
    "        research = state.research_data\n",
    "        outline = state.outline\n",
    "        \n",
    "        sections = {}\n",
    "        \n",
    "        for section_title in outline:\n",
    "            if section_title in [\"References\", \"See Also\"]:\n",
    "                # Skip these, handle separately\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Writing: {section_title}...\")\n",
    "            \n",
    "            # Generate section content based on research\n",
    "            content = self._generate_section(section_title, topic, research)\n",
    "            sections[section_title] = content\n",
    "        \n",
    "        state.sections = sections\n",
    "        state.add_message(self.name, \n",
    "                         f\"Wrote {len(sections)} sections\",\n",
    "                         sections=len(sections))\n",
    "        \n",
    "        print(f\"  âœ“ Completed {len(sections)} sections\")\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _generate_section(self, section_title, topic, research):\n",
    "        \"\"\"Generate content for a specific section\"\"\"\n",
    "        \n",
    "        # Introduction\n",
    "        if section_title == \"Introduction\":\n",
    "            articles_list = research['relevant_articles'][:3]\n",
    "            entities = list(research['key_entities'].keys())[:5]\n",
    "            \n",
    "            content = f\"**{topic}** is a significant topic in modern research and application. \"\n",
    "            content += f\"It is closely related to concepts such as {', '.join(entities[:3])}. \"\n",
    "            content += f\"This article provides a comprehensive overview of {topic}, \"\n",
    "            content += f\"drawing from multiple authoritative sources including \"\n",
    "            content += f\"{', '.join([a['title'] for a in articles_list[:2]])}.\\n\"\n",
    "            \n",
    "        # Overview\n",
    "        elif section_title in [\"Overview\", \"Definition and Overview\"]:\n",
    "            content = f\"## {section_title}\\n\\n\"\n",
    "            content += f\"{topic} encompasses several key aspects:\\n\\n\"\n",
    "            for i, entity in enumerate(list(research['key_entities'].keys())[:5], 1):\n",
    "                content += f\"- **{entity}**: A fundamental component\\n\"\n",
    "            content += f\"\\nThese elements work together to form the foundation of {topic}.\\n\"\n",
    "            \n",
    "        # History\n",
    "        elif section_title in [\"History\", \"History and Development\"]:\n",
    "            content = f\"## {section_title}\\n\\n\"\n",
    "            content += f\"The development of {topic} has evolved significantly over time. \"\n",
    "            content += f\"Key milestones include foundational research and practical applications \"\n",
    "            content += f\"that have shaped current understanding.\\n\"\n",
    "            \n",
    "        # How it Works / Principles\n",
    "        elif section_title in [\"How It Works\", \"Key Principles\", \"Key Concepts\", \"Theoretical Background\"]:\n",
    "            content = f\"## {section_title}\\n\\n\"\n",
    "            content += f\"The underlying mechanisms of {topic} involve:\\n\\n\"\n",
    "            entities = list(research['key_entities'].keys())[:6]\n",
    "            for i, entity in enumerate(entities, 1):\n",
    "                content += f\"{i}. **{entity}**: Core principle\\n\"\n",
    "            content += f\"\\nThese principles interact to produce the observed phenomena.\\n\"\n",
    "            \n",
    "        # Applications\n",
    "        elif section_title == \"Applications\":\n",
    "            content = f\"## {section_title}\\n\\n\"\n",
    "            content += f\"{topic} has found numerous practical applications:\\n\\n\"\n",
    "            categories = list(research['categories'].keys())[:4]\n",
    "            for cat in categories:\n",
    "                content += f\"- **{cat}**: Practical implementations\\n\"\n",
    "            content += f\"\\nThese applications demonstrate the versatility of {topic}.\\n\"\n",
    "            \n",
    "        # Challenges\n",
    "        elif section_title in [\"Challenges and Limitations\", \"Advantages and Disadvantages\"]:\n",
    "            content = f\"## {section_title}\\n\\n\"\n",
    "            content += f\"Despite its benefits, {topic} faces several challenges:\\n\\n\"\n",
    "            content += f\"- Complexity and scalability\\n\"\n",
    "            content += f\"- Resource requirements\\n\"\n",
    "            content += f\"- Practical constraints\\n\"\n",
    "            content += f\"\\nOngoing research aims to address these limitations.\\n\"\n",
    "            \n",
    "        # Future Directions / Current Research\n",
    "        elif section_title in [\"Future Directions\", \"Current Research\"]:\n",
    "            content = f\"## {section_title}\\n\\n\"\n",
    "            content += f\"Research in {topic} continues to evolve, with emerging areas including:\\n\\n\"\n",
    "            content += f\"- Advanced methodologies\\n\"\n",
    "            content += f\"- Novel applications\\n\"\n",
    "            content += f\"- Theoretical improvements\\n\"\n",
    "            content += f\"\\nThese directions promise to expand the impact of {topic}.\\n\"\n",
    "            \n",
    "        else:\n",
    "            # Generic section\n",
    "            content = f\"## {section_title}\\n\\n\"\n",
    "            content += f\"This section covers important aspects of {topic}. \"\n",
    "            content += f\"Further details can be found in the referenced sources.\\n\"\n",
    "        \n",
    "        return content\n",
    "\n",
    "# Initialize writing agent\n",
    "writing_agent = WritingAgent(articles)\n",
    "print(\"âœ“ Writing Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e3fb7",
   "metadata": {},
   "source": [
    "## Step 8: Implement Verification Agent\n",
    "\n",
    "Agent that adds citations and verification metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationAgent:\n",
    "    \"\"\"Agent responsible for fact-checking and citations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Verification Agent\"\n",
    "    \n",
    "    def verify(self, state: ArticleState) -> ArticleState:\n",
    "        \"\"\"Add citations and verification metadata\"\"\"\n",
    "        print(f\"\\n{self.name}: Adding citations and verification...\")\n",
    "        \n",
    "        research = state.research_data\n",
    "        articles_used = research['relevant_articles']\n",
    "        \n",
    "        # Create citations from sources\n",
    "        citations = []\n",
    "        for i, article_info in enumerate(articles_used, 1):\n",
    "            citation = {\n",
    "                'id': i,\n",
    "                'title': article_info['title'],\n",
    "                'url': article_info['url'],\n",
    "                'type': 'wikipedia',\n",
    "                'accessed': datetime.now().strftime('%Y-%m-%d')\n",
    "            }\n",
    "            citations.append(citation)\n",
    "        \n",
    "        # Verification summary\n",
    "        verification_results = {\n",
    "            'total_sources': len(articles_used),\n",
    "            'citations_added': len(citations),\n",
    "            'verification_method': 'source_based',\n",
    "            'confidence_score': 0.85,  # Simplified\n",
    "            'status': 'verified'\n",
    "        }\n",
    "        \n",
    "        state.citations = citations\n",
    "        state.verification_results = verification_results\n",
    "        state.add_message(self.name, \n",
    "                         f\"Added {len(citations)} citations\",\n",
    "                         citations=len(citations))\n",
    "        \n",
    "        print(f\"  âœ“ Added {len(citations)} citations\")\n",
    "        print(f\"  âœ“ Confidence score: {verification_results['confidence_score']:.2f}\")\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Initialize verification agent\n",
    "verification_agent = VerificationAgent()\n",
    "print(\"âœ“ Verification Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca550f",
   "metadata": {},
   "source": [
    "## Step 9: Implement Article Assembly Agent\n",
    "\n",
    "Agent that assembles the final article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssemblyAgent:\n",
    "    \"\"\"Agent responsible for assembling the final article\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Assembly Agent\"\n",
    "    \n",
    "    def assemble(self, state: ArticleState) -> ArticleState:\n",
    "        \"\"\"Assemble all sections into final article\"\"\"\n",
    "        print(f\"\\n{self.name}: Assembling final article...\")\n",
    "        \n",
    "        article_parts = []\n",
    "        \n",
    "        # Title\n",
    "        article_parts.append(f\"# {state.topic}\\n\")\n",
    "        article_parts.append(f\"*Generated by Agentic AI Wikipedia Generator*\\n\")\n",
    "        article_parts.append(f\"*Date: {datetime.now().strftime('%Y-%m-%d')}*\\n\\n\")\n",
    "        article_parts.append(\"---\\n\\n\")\n",
    "        \n",
    "        # Sections\n",
    "        for section_title in state.outline:\n",
    "            if section_title in state.sections:\n",
    "                article_parts.append(state.sections[section_title])\n",
    "                article_parts.append(\"\\n\")\n",
    "        \n",
    "        # See Also\n",
    "        article_parts.append(\"## See Also\\n\\n\")\n",
    "        for i, article in enumerate(state.research_data['relevant_articles'][:5], 1):\n",
    "            article_parts.append(f\"- [{article['title']}]({article['url']})\\n\")\n",
    "        article_parts.append(\"\\n\")\n",
    "        \n",
    "        # References\n",
    "        article_parts.append(\"## References\\n\\n\")\n",
    "        for citation in state.citations:\n",
    "            article_parts.append(\n",
    "                f\"[{citation['id']}] {citation['title']}. \"\n",
    "                f\"Wikipedia. Retrieved {citation['accessed']}. \"\n",
    "                f\"{citation['url']}\\n\\n\"\n",
    "            )\n",
    "        \n",
    "        # Metadata footer\n",
    "        article_parts.append(\"\\n---\\n\\n\")\n",
    "        article_parts.append(\"### Generation Metadata\\n\\n\")\n",
    "        article_parts.append(f\"- **Sources Used**: {len(state.citations)}\\n\")\n",
    "        article_parts.append(f\"- **Sections**: {len(state.sections)}\\n\")\n",
    "        article_parts.append(f\"- **Key Entities**: {len(state.research_data['key_entities'])}\\n\")\n",
    "        article_parts.append(f\"- **Confidence Score**: {state.verification_results['confidence_score']:.2f}\\n\")\n",
    "        article_parts.append(f\"- **Status**: {state.verification_results['status']}\\n\")\n",
    "        \n",
    "        final_article = \"\".join(article_parts)\n",
    "        state.final_article = final_article\n",
    "        state.add_message(self.name, \n",
    "                         f\"Assembled article with {len(final_article)} characters\",\n",
    "                         length=len(final_article))\n",
    "        \n",
    "        print(f\"  âœ“ Article assembled: {len(final_article)} characters\")\n",
    "        print(f\"  âœ“ Includes {len(state.sections)} content sections\")\n",
    "        print(f\"  âœ“ With {len(state.citations)} citations\")\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Initialize assembly agent\n",
    "assembly_agent = AssemblyAgent()\n",
    "print(\"âœ“ Assembly Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedbd337",
   "metadata": {},
   "source": [
    "## Step 10: Create Agent Orchestrator\n",
    "\n",
    "Orchestrator that coordinates the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f4c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleOrchestrator:\n",
    "    \"\"\"Orchestrates the multi-agent workflow\"\"\"\n",
    "    \n",
    "    def __init__(self, research_agent, planning_agent, writing_agent, \n",
    "                 verification_agent, assembly_agent):\n",
    "        self.research_agent = research_agent\n",
    "        self.planning_agent = planning_agent\n",
    "        self.writing_agent = writing_agent\n",
    "        self.verification_agent = verification_agent\n",
    "        self.assembly_agent = assembly_agent\n",
    "    \n",
    "    def generate_article(self, topic: str) -> ArticleState:\n",
    "        \"\"\"Execute the complete article generation pipeline\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ARTICLE GENERATION PIPELINE\")\n",
    "        print(f\"Topic: {topic}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Initialize state\n",
    "        state = ArticleState(topic=topic)\n",
    "        \n",
    "        # Execute agents in sequence\n",
    "        state = self.research_agent.research(state)\n",
    "        state = self.planning_agent.plan(state)\n",
    "        state = self.writing_agent.write(state)\n",
    "        state = self.verification_agent.verify(state)\n",
    "        state = self.assembly_agent.assemble(state)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PIPELINE COMPLETE!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Generated article: {len(state.final_article)} characters\")\n",
    "        print(f\"Total agents executed: 5\")\n",
    "        print(f\"Messages exchanged: {len(state.messages)}\")\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Initialize orchestrator\n",
    "orchestrator = ArticleOrchestrator(\n",
    "    research_agent=research_agent,\n",
    "    planning_agent=planning_agent,\n",
    "    writing_agent=writing_agent,\n",
    "    verification_agent=verification_agent,\n",
    "    assembly_agent=assembly_agent\n",
    ")\n",
    "\n",
    "print(\"âœ“ Orchestrator initialized and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a681bc5",
   "metadata": {},
   "source": [
    "## Step 11: Generate Test Article\n",
    "\n",
    "Let's generate our first Wikipedia-style article!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an article\n",
    "test_topic = \"Deep Learning\"\n",
    "\n",
    "result_state = orchestrator.generate_article(test_topic)\n",
    "\n",
    "# Display a preview\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ARTICLE PREVIEW (First 1000 characters)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "print(result_state.final_article[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e8fb7",
   "metadata": {},
   "source": [
    "## Step 12: Display Complete Article\n",
    "\n",
    "View the complete generated article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4989c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the full article\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(result_state.final_article))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb170fe3",
   "metadata": {},
   "source": [
    "## Step 13: Save Generated Article\n",
    "\n",
    "Save the article to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ac5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save article to file\n",
    "article_filename = OUTPUTS_DIR / f\"{test_topic.replace(' ', '_')}_generated.md\"\n",
    "with open(article_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(result_state.final_article)\n",
    "\n",
    "print(f\"âœ“ Article saved to: {article_filename}\")\n",
    "print(f\"  File size: {article_filename.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d00c1",
   "metadata": {},
   "source": [
    "## Step 14: Generate Multiple Articles\n",
    "\n",
    "Generate articles for different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744aa194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate articles for multiple topics\n",
    "test_topics = [\n",
    "    \"Machine Learning\",\n",
    "    \"Quantum Computing\",\n",
    "    \"Natural Language Processing\"\n",
    "]\n",
    "\n",
    "generated_articles = {}\n",
    "\n",
    "for topic in test_topics:\n",
    "    print(f\"\\nGenerating article for: {topic}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    state = orchestrator.generate_article(topic)\n",
    "    generated_articles[topic] = state\n",
    "    \n",
    "    # Save article\n",
    "    filename = OUTPUTS_DIR / f\"{topic.replace(' ', '_')}_generated.md\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(state.final_article)\n",
    "    \n",
    "    print(f\"âœ“ Saved: {filename}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Generation Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total articles generated: {len(generated_articles)}\")\n",
    "print(f\"Articles saved to: {OUTPUTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c7dd9b",
   "metadata": {},
   "source": [
    "## Step 15: Analyze Generation Statistics\n",
    "\n",
    "Analyze the article generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707720cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis summary\n",
    "analysis_data = []\n",
    "\n",
    "for topic, state in generated_articles.items():\n",
    "    analysis_data.append({\n",
    "        'Topic': topic,\n",
    "        'Length (chars)': len(state.final_article),\n",
    "        'Sections': len(state.sections),\n",
    "        'Citations': len(state.citations),\n",
    "        'Sources': state.research_data['total_sources'],\n",
    "        'Key Entities': len(state.research_data['key_entities']),\n",
    "        'Confidence': state.verification_results['confidence_score'],\n",
    "        'Agent Messages': len(state.messages)\n",
    "    })\n",
    "\n",
    "analysis_df = pd.DataFrame(analysis_data)\n",
    "\n",
    "print(\"Article Generation Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "print(analysis_df.to_string(index=False))\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  Average article length: {analysis_df['Length (chars)'].mean():.0f} characters\")\n",
    "print(f\"  Average sections per article: {analysis_df['Sections'].mean():.1f}\")\n",
    "print(f\"  Average citations per article: {analysis_df['Citations'].mean():.1f}\")\n",
    "print(f\"  Average sources used: {analysis_df['Sources'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a93ad",
   "metadata": {},
   "source": [
    "## Step 16: Project Summary\n",
    "\n",
    "Final summary of the complete system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d97e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROJECT COMPLETE: Agentic AI-Powered Wikipedia Article Generator\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"âœ… Phase 1: Data Collection & Preprocessing\")\n",
    "print(f\"   - Collected 1,337 Wikipedia articles\")\n",
    "print(f\"   - Extracted entities and relationships\")\n",
    "print(f\"   - Built knowledge graph with 11,091 edges\")\n",
    "print(f\"   - Generated 384-dim embeddings\")\n",
    "\n",
    "print(\"\\nâœ… Phase 2: GraphRAG Engine\")\n",
    "print(f\"   - Built FAISS vector index\")\n",
    "print(f\"   - Implemented graph traversal\")\n",
    "print(f\"   - Created hybrid retrieval system\")\n",
    "print(f\"   - Fusion ranking algorithm\")\n",
    "\n",
    "print(\"\\nâœ… Phase 3: Multi-Agent System\")\n",
    "print(f\"   - Research Agent: Information gathering\")\n",
    "print(f\"   - Planning Agent: Article structuring\")\n",
    "print(f\"   - Writing Agent: Content generation\")\n",
    "print(f\"   - Verification Agent: Citations & validation\")\n",
    "print(f\"   - Assembly Agent: Final compilation\")\n",
    "print(f\"   - Orchestrator: Workflow coordination\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"System Capabilities:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"âœ“ Automatic article generation from topics\")\n",
    "print(\"âœ“ Multi-source information retrieval\")\n",
    "print(\"âœ“ Graph-based + semantic search\")\n",
    "print(\"âœ“ Structured content with citations\")\n",
    "print(\"âœ“ Verification metadata\")\n",
    "print(\"âœ“ Scalable agent architecture\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Generated Articles:\")\n",
    "print(f\"{'='*80}\")\n",
    "for topic in generated_articles.keys():\n",
    "    filename = OUTPUTS_DIR / f\"{topic.replace(' ', '_')}_generated.md\"\n",
    "    print(f\"  â€¢ {topic}: {filename}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ðŸŽ‰ All phases complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
